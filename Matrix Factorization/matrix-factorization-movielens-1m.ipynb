{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Original Notebook can be found in [here](https://www.kaggle.com/code/phamdinhkhanh/matrix-factorization-movie-length-1m/notebook)"]},{"cell_type":"markdown","metadata":{"_uuid":"4308205b065c7e6684592ec42e475e5bcbe9b773"},"source":["## 1. Introduction to the Recommendation system method\n","### 1. 1. The importance of recommendation system\n","In everyday life, we often see quite accidental situations when large systems have the ability to read and understand user preferences and display information that users are interested in very accurately. Such as:\n","\n","* Facebook has the ability to display statuses of people you care about on your newsfeed.\n","* Youtube can automatically jump to videos you're likely to like based on what you're watching.\n","* Amazon may offer books in the same category as the books you have purchased or with high ratings.\n","* Google may show an ad for an item that matches your recent searches.\n","\n","Without recommendation algorithms, the user experience will be poorer because the information they are really interested in is not presented at the right time while unnecessary information is presented more often. As a result, users feel annoyed and have confusing information. In the field of marketing, recommendation systems become even more important. Each product delivered to the right consumer will increase revenue, reduce time costs, advertising costs and help consumers own what they need. In the field of entertainment such as videos, games, online stories,... users will achieve high satisfaction when finding the right type of entertainment they love easily. Although it was only born in the last 10 years, along with the internet boom, it can be said that `recommendation system` is a vibrant research field. It has created a revolution that has changed shopping behavior, entertainment behavior, business strategies,... globally. Hundreds of millions of small businesses are benefiting from it by exploiting endless customer sources from network resources and turning this sales channel into a replacement for traditional channels. This field is also the key to helping technology companies Google, Facebook, Amazon, Microsoft,... become the world's leading corporations. That's why 'recommendation system' is always invested in research and development by online business companies to create a smart system to improve customer experience and optimize resources.\n","\n","### 1.2. Recommendation method\n","\n","Above we know the role of the `recommendation system` in the development of the internet and online business. But what really is the `recommendation system` problem? We still don't really understand the basis of the `recommendation system` method. According to the definition from wikipedia, `recommendation system` is a small branch of the field of *information filtering system* (information filtering system) used to predict the level of popularity through the rating of a user. for a product (item). To offer the most suitable products to users requires systems to rely on product rating information, user information, and product information to build optimal algorithms. Based on the loss function to calculate the algorithm's prediction error and find a method with the most accurate prediction level. There are many different algorithms used in the `recommendation system` but they basically include two main methods: `Collaborative filtering` and `Content based filtering`. The basic difference between these two methods is:\n","\n","* Collaborative filtering: Based on correlations in consumer behavior or product characteristics to find users or items with similar characteristics and interests. From there, based on the information that the user group or the most recent related product has rated, to evaluate the product that a specific user has not rated. However, the disadvantage of this algorithm is that it makes predictions about ratings without fully understanding the users and items, but is completely based on observations of the equivalence between groups of users and items for prediction. The algorithm used in these problems is mainly k-nearest neighbor to find equivalence groups and the correlation coefficient matrix is used to measure the degree of behavioral or characteristic closeness for grouping. .\n","\n","* Content based filtering: Based on information and content related to the product such as manufacturer, genre, year of manufacture, uses, characteristics,... or based on user information such as gender , age, industry,... to make predictions about people's ratings for that product. This algorithm is simply a regression equation between the dimensions of product or user characteristics against rating scores without taking advantage of behavioral correlations between user groups or product characteristics such as ` Collaborative filtering`. In fact, user behavior shows that they are very similar if they belong to the same group, such as meditation music, gold music, youth music, and children's music groups, which will be suitable for the elderly, middle-aged people, young people, etc. Children. Not considering group correlation factors is a major limitation of content based filtering.\n","\n","\n","Each algorithm has different advantages and disadvantages, and the level of effectiveness in predicting the popularity of (user, item) pairs (*user, product*) also varies depending on the data set. But the algorithms all have in common that they use data that users have rated products as a basis for predicting ratings for unrated products. This is like playing a game of filling in numbers into the *utility matrix*. One dimension of the matrix corresponds to users and the other dimension corresponds to items. The cells on the matrix represent the user's rating value corresponding to the item. Thus, there will be cells that have been rated by users and the remaining cells that have not been rated. The process of solving the problem is like solving the matrix in the missing cells so that the final error between forecast and reality is smallest.\n","\n","### 1.3. Introducing the matrix factorization algorithm\n","\n","\n","In the matrix factorization algorithm, we assume the item's characteristics are represented by the matrix $\\mathbf{I}$ and the user's behavior is represented by the matrix $\\mathbf{U}$. For each row of the matrix $\\mathbf{I}$ is a latent feature (*latent feature*) of the product and each column of $\\mathbf{U}$ is a user's preference for the feature. corresponding hidden. These hidden characteristics can be considered as principal factors aggregated from various product-related information similar to principal components in principal component analysis `PCA`. The characteristics of the mth product are represented by the line vector $\\mathbf{i_m}$ and the behavior of the nth user is represented by the column vector $\\mathbf{u_n}$. Then the predictive value of a user n's preference for a product m will be the product of two vectors $\\mathbf{i_m}$ and $\\mathbf{u_n}$:\n","\\begin{equation*}\n","y_{mn} = \\mathbf{i_m} \\mathbf{u_n}\n","\\end{equation*}\n","\n","The estimate of the utility matrix $\\mathbf{\\hat{Y}}$ will be expressed in terms of the behavior matrix $\\mathbf{I}$ and the user matrix $\\mathbf{U}$ as follows:\n","\n","\n","\\begin{equation*}\n","\\mathbf{\\hat{Y}} \\approx \\left[ \\begin{matrix}\n","\\mathbf{i}_1\\mathbf{u}_1 & \\mathbf{i}_1\\mathbf{u}_2 & \\dots & \\mathbf{i}_1 \\mathbf{u}_N\\\\\n","\\mathbf{i}_2\\mathbf{u}_1 & \\mathbf{i}_2\\mathbf{u}_2 & \\dots & \\mathbf{i}_2 \\mathbf{u}_N\\\\\n","\\dots & \\dots & \\ddots & \\dots \\\\\n","\\mathbf{i}_M\\mathbf{u}_1 & \\mathbf{i}_M\\mathbf{u}_2 & \\dots & \\mathbf{i}_M \\mathbf{u}_N\\\\\n","\\end{matrix} \\right]\n"," = \\left[ \\begin{matrix}\n","\\mathbf{i}_1 \\\\\n","\\mathbf{i}_2 \\\\\n","\\dots \\\\\n","\\mathbf{i}_M \\\\\n","\\end{matrix} \\right]\n","\\left[ \\begin{matrix}\n","\\mathbf{u}_1 & \\mathbf{u}_2 & \\dots & \\mathbf{u}_N\n","\\end{matrix} \\right] = \\mathbf{IU}\n","\\end{equation*}\n","\n","\n","### 1.4. Gradient descent algorithm\n","\n","Assuming that we already have information about the matrices $\\mathbf{U}$ and $\\mathbf{I}$ what we need to do now is consider the rows of each $\\mathbf{I}$ as a profile item and each column of $\\mathbf{U}$ is a user profile. Suppose $\\mathbf{I} \\in \\mathbb{R}^{M \\times K}$, $\\mathbf{U} \\in \\mathbb{R}^{K \\times N}$, $\\mathbf{ Y} \\in \\mathbb{R}^{M \\times N}$. Normally we will choose the number of hidden features smaller than the number of products and users. Then Y will be expressed as the product of 2 matrices with lower rank (*Low-rank Matrix factorization*):\n","\n","\\begin{aligned}\\hat{\\mathbf{Y}} = \\mathbf{I}\\mathbf{U}\\end{aligned}\n","\n","The loss function of the algorithm is the standard [Frobenius norm](http://mathworld.wolfram.com/FrobeniusNorm.html) of the difference between $\\mathbf{Y}$ and $\\mathbf{\\hat{Y}} $ as follows:\n","\n","\\begin{aligned}\n","\\mathcal{L(\\mathbf{I},\\mathbf{U})} = \\frac{1}{2s}||\\mathbf{Y}-\\mathbf{\\hat{Y}}||_{F} ^2\n","\\end{aligned}\n","\n","To avoid overfitting [2nd order correction factor](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) (*l2 - reguralization*) is added:\n","\n","\\begin{equation*}\n","\\mathcal{L(\\mathbf{I},\\mathbf{U})} = \\frac{1}{2s}||\\mathbf{Y}-\\mathbf{\\hat{Y}}||_{F}^2 + \\frac{\\lambda_1}{2}||\\mathbf{I}||_{F}^2 + \\frac{\\lambda_2}{2}||\\mathbf{U}||_{F}^2             \\tag{1.4.1}\n","\\end{equation*}\n","\n","If $\\mathbf{U}$ is considered fixed and $\\mathbf{I}$ needs to be optimized. The `Matrix factorization` problem would be equivalent to optimizing the loss function:\n","\n","\\begin{aligned}\n","\\mathcal{L(\\mathbf{I})} = \\frac{1}{2s}||\\mathbf{Y}-\\mathbf{\\hat{Y}}||_{F}^2 + \\frac{ \\lambda_1}{2}||\\mathbf{I}||_{F}^2\n","\\end{aligned}\n","\n","If $\\mathbf{I}$ is considered fixed and $\\mathbf{U}$ needs to be optimized. The loss function will have the form:\n","\n","\\begin{aligned}\n","\\mathcal{L(\\mathbf{U})} = \\frac{1}{2s}||\\mathbf{Y}-\\mathbf{\\hat{Y}}||_{F}^2 + \\frac{ \\lambda_2}{2}||\\mathbf{U}||_{F}^2\n","\\end{aligned}\n","\n","We see that the loss functions are all convex functions. Finding the optimal solution can be based on the quadratic convex optimization problem (*Quadratic Programming*) or a simpler way is through the `gradient descent` algorithm. We will use the `Stochastic gradient descent` algorithm to update each data point in turn across the entire data, then repeat this process. In the case of a fixed product matrix ($\\mathbf{I}$), we will need to update the user matrix ($\\mathbf{U}$) according to gradient descent. For each update, a user u is selected. Based on information about products that users have rated. Vector gradient descent is calculated to update the value of the corresponding vector $\\mathbf{u}$. This process continues until all users vectors are updated. The same goes for the case of fixing the fixed user matrix and updating the product matrix.\n","\n","**Optimize user matrix:**\n","\n","To simplify the calculation process, we can express the loss function according to the total loss function of each user as follows:\n","\n","\\begin{aligned}\n","\\mathcal{L(\\mathbf{U})} = \\frac{1}{2s}\\sum_{n = 1}^{N}\\sum_{m: r_{mn} = 1} (y_{mn} - i_m . u_n)^2+\\frac{\\lambda_1}{2} ||\\mathbf{U}||_{F}^2\n","\\end{aligned}\n","\n","Where $r_{mn}$ is an element of the rating matrix $R \\in \\mathbb{R}^{M \\times N}$ with value 0 or 1. $r_{mn} = 1$ marks the product Product m has been rated by user n and is 0 in case it has not been rated. The condition $r_{mn} = 1$ in the loss function is to filter out products that have been rated by user n. Then, if we consider $\\hat{\\mathbf{I}}_n$ to be the matrix of rated products of user n and $\\hat{y}_n$ to be the corresponding rating result vector, then the loss function for user n is abbreviated as follows:\n","\n","\\begin{aligned}\n","\\mathcal{L(\\mathbf{U}| user = n)} = \\frac{1}{2s}\\sum_{m: r_{mn} = 1}(y_{mn} - \\mathbf{i_m} . \\ mathbf{u_n})^2 + \\frac{\\lambda_1}{2}||\\mathbf{u_n}||^2 = \\frac{1}{2s}||\\mathbf{\\hat{y}_n}- \\hat{\\mathbf{I}}_n. \\mathbf{u_n}||^2 + \\frac{\\lambda_1}{2}||\\mathbf{u_n}||^2\n","\\end{aligned}\n","\n","Its derivative corresponds to:\n","\n","\\begin{aligned}\n","\\frac{\\partial \\mathcal{L(\\mathbf{U}| user = n)}}{\\partial \\mathbf{u_n}} = -\\frac{1}{s}\\hat{\\mathbf{I}} _n^{T}\\space (\\mathbf{\\hat{y}_n}-\\hat{\\mathbf{I}}_n. \\mathbf{u_n}) + \\lambda_1\\mathbf{u_n}\n","\\end{aligned}\n","\n","Formula to update the solution for each column of the user matrix:\n","\n","\\begin{aligned}\n","\\mathbf{u'_n} = \\mathbf{u_n} - \\theta (-\\frac{1}{s}\\hat{\\mathbf{I}}_m^{T}\\space (\\mathbf{\\hat{y }_n}-\\hat{\\mathbf{I}}_n. \\mathbf{u_n}) + \\lambda_1\\mathbf{u_n})\n","\\end{aligned}\n","\n","**Optimize product matrix:**\n","\n","Completely similar, we also have for the product matrix, the loss function for item = m:\n","\n","\\begin{aligned}\n","\\mathcal{L(\\mathbf{I}| item = m)} = \\frac{1}{2s}\\sum_{n: r_{mn} = 1}(y_{mn} - \\mathbf{i_m} . \\mathbf{u_n})^2 + \\frac{\\lambda_1}{2}||\\mathbf{i_m}||^2 = \\frac{1}{2s}||\\mathbf{\\hat{y}_m}-\\mathbf{i_m}.\\hat{\\mathbf{U}}_m||^2 + \\frac{\\lambda_1}{2}||\\mathbf{i_m}||^2\n","\\end{aligned}\n","\n","The corresponding derivative for each item will be:\n","\n","\\begin{aligned}\n","\\frac{\\partial \\mathcal{L(\\mathbf{I}| item = m)}}{\\partial \\mathbf{i_m}} = -\\frac{1}{s}(\\mathbf{\\hat{y}_m}-\\mathbf{i_m}. \\hat{\\mathbf{U}}_m)\\hat{\\mathbf{U}}_m^{T} + \\lambda_1\\mathbf{i_m}\n","\\end{aligned}\n","\n","Formula to update the solution for each row of the product matrix:\n","\n","\\begin{aligned}\n","\\mathbf{i'_m} = \\mathbf{i_m} - \\theta (-\\frac{1}{s}(\\mathbf{\\hat{y}_m}-\\mathbf{i_m}. \\hat{\\mathbf{U}}_m)\\hat{\\mathbf{U}}_m^{T} + \\lambda_1\\mathbf{i_m})\n","\\end{aligned}\n","\n","## 2. Build algorithm code\n","\n","### 2.1. Practice on the movie length 1M data set\n","\n","We will perform the `matrix factorization` method on the [Movie length 1M](http://files.grouplens.org/datasets/movielens/ml-1m.zip) data set of 1 million ratings for about 4000 Movies are collected from 6000 users. To facilitate compliance with the developed algorithm, data processing will be performed on the matrix. Load input data as follows:\n"]},{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-input":true,"_uuid":"75921cc9eeeb8a254be0f53c590f91911f755eba","execution":{"iopub.execute_input":"2024-04-25T15:57:20.786199Z","iopub.status.busy":"2024-04-25T15:57:20.785884Z","iopub.status.idle":"2024-04-25T15:57:28.541808Z","shell.execute_reply":"2024-04-25T15:57:28.541270Z","shell.execute_reply.started":"2024-04-25T15:57:20.786134Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>item_id</th>\n","      <th>rating</th>\n","      <th>timestamp</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>39</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>978824268</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>1</td>\n","      <td>48</td>\n","      <td>5</td>\n","      <td>978824351</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>1</td>\n","      <td>150</td>\n","      <td>5</td>\n","      <td>978301777</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>1</td>\n","      <td>260</td>\n","      <td>4</td>\n","      <td>978300760</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>1</td>\n","      <td>527</td>\n","      <td>5</td>\n","      <td>978824195</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    user_id  item_id  rating  timestamp\n","39        1        1       5  978824268\n","24        1       48       5  978824351\n","38        1      150       5  978301777\n","43        1      260       4  978300760\n","22        1      527       5  978824195"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","columns = ['user_id', 'item_id', 'rating', 'timestamp']\n","movie_length = pd.read_csv('../input/ratings.dat', header = 0, \\\n","                           names = columns, sep = '::', engine = 'python')\n","movie_length = movie_length.sort_values(['user_id', 'item_id'])\n","movie_length.head()"]},{"cell_type":"markdown","metadata":{"_uuid":"2e93ea4ee0079bbb43581888a5f27652884cfb1f"},"source":["Size of data and number of users and items"]},{"cell_type":"code","execution_count":2,"metadata":{"_uuid":"c0d5eda977a1b49630b92453ddce67895034afb5","execution":{"iopub.execute_input":"2024-04-25T15:57:28.543491Z","iopub.status.busy":"2024-04-25T15:57:28.543197Z","iopub.status.idle":"2024-04-25T15:57:28.606481Z","shell.execute_reply":"2024-04-25T15:57:28.605721Z","shell.execute_reply.started":"2024-04-25T15:57:28.543435Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Data movie length shape: (1000208, 4)\n","No customers: 6040\n","No movies: 3706\n"]}],"source":["print('Data movie length shape: %s'%str(movie_length.shape))\n","print('No customers: %s'%str(np.unique(movie_length.iloc[:, 0]).shape[0]))\n","print('No movies: %s'%str(np.unique(movie_length.iloc[:, 1]).shape[0]))"]},{"cell_type":"markdown","metadata":{"_uuid":"885ac3a9ba3dcb7150cdcda80a8ffefaab052c86"},"source":["Statistics describing the rating frequency of users:"]},{"cell_type":"code","execution_count":3,"metadata":{"_uuid":"ac953acef011c01c50a1ac9aef9a2ee1cac2730f","execution":{"iopub.execute_input":"2024-04-25T15:57:28.609640Z","iopub.status.busy":"2024-04-25T15:57:28.609426Z","iopub.status.idle":"2024-04-25T15:57:28.631736Z","shell.execute_reply":"2024-04-25T15:57:28.631112Z","shell.execute_reply.started":"2024-04-25T15:57:28.609602Z"},"trusted":true},"outputs":[{"data":{"text/plain":["count    6040.000000\n","mean      165.597351\n","std       192.747126\n","min        20.000000\n","25%        44.000000\n","50%        96.000000\n","75%       208.000000\n","max      2314.000000\n","Name: user_id, dtype: float64"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["movie_length['user_id'].value_counts().describe()"]},{"cell_type":"markdown","metadata":{"_uuid":"5d527f28e23916ff8f4628fe2937578234828e2a"},"source":["* On average, one user rates a total of 165 movies.\n","* The lowest user rates 20 movies and the highest user rates 2314 movies.\n","* The range of popular rating movies per user is from 44 to 208 movies (accounting for 50%)."]},{"cell_type":"code","execution_count":4,"metadata":{"_uuid":"76d03c224285779b0288471bf356b364a1569a9d","execution":{"iopub.execute_input":"2024-04-25T15:57:28.633058Z","iopub.status.busy":"2024-04-25T15:57:28.632813Z","iopub.status.idle":"2024-04-25T15:57:29.032378Z","shell.execute_reply":"2024-04-25T15:57:29.031369Z","shell.execute_reply.started":"2024-04-25T15:57:28.633017Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Text(0,0.5,'No customers')"]},"execution_count":4,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAt0AAAHwCAYAAAB67dOHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XuYZVV95//3Ry5qALkE7SCgjUoyokTUHsSYS3kJAiZBk6gYVEBm0PlhvGFim0kCYpyQTJB5GJWZdrh6Q+IltoDR9lJjnIgChqvI0D9ooaWFINdGJTZ+54+9S08Xp6pPN7Wquqrer+c5T52zztprrXPO6upP7bP23qkqJEmSJLXziLkegCRJkrTQGbolSZKkxgzdkiRJUmOGbkmSJKkxQ7ckSZLUmKFbkiRJaszQLWkkSf5Hkr+YobaekGR9km36x+NJ/sNMtN2397kkR81Ue5vR718luSPJ92e7782V5DeSXD/LfR6d5Guz2efm2NrHJ2l+M3RLIsmaJD9Kcl+Su5P8c5I3JPnZ74iqekNVvXvEtl40XZ2qurmqdqyqB2dg7Ccl+fCk9g+tqnMfbtubOY69gROA/arql2az71EkqSRPmXhcVf9UVb8yl2PSxgz90sJm6JY04XeraifgicApwDuAM2e6kyTbznSbW4knAj+oqttnu+MF/J5qjjinpJln6Ja0kaq6p6pWAq8EjkrydIAk5yT5q/7+7kku7PeK35nkn5I8IsmHgCcAn+2Xj/xpkqX9XtZjk9wMfHmgbPA/9icn+WaSe5J8JslufV9jSdYOjnFib3qSQ4A/A17Z93dl//zPlqv04/rzJN9NcnuS85Ls3D83MY6jktzcLw35z1O9N0l27rf/1769P+/bfxGwCnh8P45zhmw7lmRtkhP6caxLcsym2p5iHCcl+USSDye5Fzg6yYFJvt5/JuuSvC/J9n39r/abXtmP75WT39f+PX17kqv6z+DjSR418Pyf9u3emuQ/DO45T3JYkm/335R8L8nbp3oPu+r5730f30nywr7w5Ukun1TxhCT/MM1ncWY/pu+lW9ozsVzpyUm+nOQH/Wf6kSS7DGy7d5JP9e/1D5K8b1Lbf5fkriQ3JTl0mhcytJ1M+vZl8nxPt0f7xv79uinJkUmeCvwP4Ln9Z3T3wOscOi/6dv5PktP6z/3GJL/Wl9/Sz7OjBsbxyP613ZzktnRLxh7dPzcxP9+RbnnU2dN8hpK2gKFb0lBV9U1gLfAbQ54+oX/uscASuuBbVfUa4Ga6veY7VtXfDmzzW8BTgRdP0eVrgdcBjwc2AKePMMZ/BP4L8PG+v2cMqXZ0f3s+8CRgR+B9k+r8OvArwAuBv+wD0DD/Hdi5b+e3+jEfU1VfBA4Fbu3HcfQU2/9Sv/2ewLHA+5PsOl3bU7124HDgE8AuwEeAB4G3ArsDz+1fy/8HUFW/2W/zjH58H5+izVcAhwD7AL9K976R7o+btwEvAp7Sj2/QmcDr+29Kng58eZpxPwe4sR/nicCn0v2BtRLYZ9J7/2rgQ1O0cy7dPHkK8EzgYGDiuIAAf003l54K7A2c1L+WbYALge8CS+k+i/Mnje/6fnx/C5yZJJM7H6GdoZLsQDe3D+3fr18Drqiq64A3AF/vP6OJPxI2NS+eA1wF/CLw0X4M/75/X14NvC/Jjn3dvwF+GTigf35P4C8H2volYDe6b22O29RrkbR5DN2SpnMr3X/Ck/0E2AN4YlX9pF8fXJto66Squr+qfjTF8x+qqmuq6n7gL4BXTOy5fJiOBN5bVTdW1XrgncAR2Xgv+7uq6kdVdSVwJfCQ8N6P5ZXAO6vqvqpaA5wKvGYzxvIT4OT+PbsYWA/8yha2/fWq+oeq+mk/9sur6pKq2tBv/z95aDjelNOr6taquhP4LF04gy6Mn11V11bVD4F3DXld+yV5TFXdVVXfmqaP24H/1r8HH6cLuC+pqgeAj9MFRZI8jS7MXji5gSRL6P7IeUs/p24HTgOOAKiq1VW1qqoeqKp/Bd478F4cSBfG/6Tf9sdVNbiO+rtV9cH+eINz6eb5kiGvY1PtTOenwNOTPLqq1lXVtcMqjTgvbqqqs/vxfpzuD4yT+9f+BeDfgKf0fzj8R+CtVXVnVd1H9wfrEZPGdWK/7VT/TiVtIUO3pOnsCdw5pPy/AquBL/RfaS8foa1bNuP57wLb0e1tfLge37c32Pa2bBykBs828kO6veGT7Q5sP6StPTdjLD+oqg1D+tqStjd6P5P8crolP99Pt+Tkv7D5799U78PjJ/U3+bP8A+Aw4LtJ/neS507Tx/cm/YH23b596ELuH/UB8TXABX0Yn+yJdPNjXb+s4m66PzIeB5DkcUnO75ed3At8mJ+/F3vTBesNQ9qFgfeg/wMDhs+HTbUzVP9H5Svp9mqvS3JRkn83RfVR5sVtA/d/1PcxuWxHum+lfgG4fOA9+8e+fMK/VtWPN+f1SBqdoVvSUEn+Pd1/7g/Ze9fvdTuhqp4E/C7wtom1ucBUe7w3tSd874H7T6Dbe3oHcD9dWJgY1zZsHBQ21e6tdCFtsO0NbBxWRnFHP6bJbX1vM9uZqbYnv+4zgO8A+1bVY+iW/DxkWcQWWgfsNfB48LOiqi6tqsPpQu8/ABdM09aek5ZrPIHuM6KqLqHbM/sbwB8x9dKSW4AHgN2rapf+9piqelr//F/TvT+/2r8Xr+bn78UtwBPy8A8UnK6djeYs3bKNn6mqz1fVb9PtRf8O8MGJpya1M5Nz7g66AP60gfds56oa/INiU/+WJD0Mhm5JG0nymCS/Q7c29MNVdfWQOr+TZOIr63vp1hNPnP7vNrr1p5vr1Un2S/ILwMnAJ/qvzP8v8KgkL0myHfDnwCMHtrsNWJopDjoEPga8Nck+/drWiTXgm7uH8kG6MPmeJDsleSLdOucPT7/lrLW9E91nsb7fc/qfJj2/pZ8L/diOSfLU/vP52TrgJNv3BwLuXFU/4efzYSqPA96UZLskL6dbc33xwPPn0a253zDVco2qWgd8ATi1n6+PSHfw5MQSkp3olu7cnWRP4E8GNv8m3R8RpyTZIcmjkjxv9LdipHauAH4z3fnod6Zb0gR0S2OS/F6/tvuBfpyD/3b2Sn8A7EzOuar6KV24Py3JxDcCeyaZ6hgLSTPM0C1pwmeT3Ee3B+8/062DnepAvn2BL9IFhq8DH6iq8f65vwb+vP8Ke7qzWEz2IeAcuq/3HwW8CbqzqdAdEPi/6Pbw3U93EOeEv+9//iDJsLXEZ/VtfxW4Cfgx8MebMa5Bf9z3fyPdNwAf7dufCQ+37bfT7R2+jy5cTT5Y8iTg3P5zecXmDKyqPkd38N9X6JYVfb1/amLpx2uANf1SjjfQr8uewjfo5s8dwHuAP6yqHww8/yG6gzGn2ss94bV0Sy++DdxFd1DpHv1z7wKeBdwDXAR8auC1PEj37cxT6A76XUu33GOzTNdOVa2ie/+vAi5n43Xpj6A7EPlWuqVbv0V/wCvdAajXAt9PckdfNpNz7h10n98l/Wf1RboDiCXNgmz62CdJkn6uP8PINcAjN/cbgxHafjTdwZbPqqobZrJtSZpL7umWJG1Skpf1S0l2pTv13GdnOnD3/hNwqYFb0kLjFackSaN4Pd3ynweB/83Pl0TMmCRr6A54fOlMty1Jc83lJZIkSVJjLi+RJEmSGjN0S5IkSY0tyDXdu+++ey1durR5P/fffz877LBD8340PzgfNJlzQpM5JzTI+bAwXH755XdU1WM3VW9Bhu6lS5dy2WWXNe9nfHycsbGx5v1ofnA+aDLnhCZzTmiQ82FhSPLdUeq5vESSJElqzNAtSZIkNWboliRJkhozdEuSJEmNGbolSZKkxgzdkiRJUmOGbkmSJKkxQ7ckSZLUmKFbkiRJaszQLUmSJDVm6JYkSZIaM3RLkiRJjRm6JUmSpMYM3ZIkSVJjhm5JkiSpMUO3JEmS1JihW5IkSWrM0C1JkiQ1ZuiWJEmSGtt2rgew0CxdftGc9LvmlJfMSb+SJEnaNPd0S5IkSY0ZuiVJkqTGDN2SJElSY4ZuSZIkqTFDtyRJktSYoVuSJElqzNAtSZIkNWboliRJkhozdEuSJEmNGbolSZKkxgzdkiRJUmOGbkmSJKkxQ7ckSZLUmKFbkiRJaszQLUmSJDVm6JYkSZIaM3RLkiRJjRm6JUmSpMYM3ZIkSVJjhm5JkiSpMUO3JEmS1JihW5IkSWqsWehO8qgk30xyZZJrk7yrLz8nyU1JruhvB/TlSXJ6ktVJrkryrIG2jkpyQ387qtWYJUmSpBa2bdj2A8ALqmp9ku2AryX5XP/cn1TVJybVPxTYt789BzgDeE6S3YATgWVAAZcnWVlVdzUcuyRJkjRjmu3prs76/uF2/a2m2eRw4Lx+u0uAXZLsAbwYWFVVd/ZBexVwSKtxS5IkSTOt6ZruJNskuQK4nS44f6N/6j39EpLTkjyyL9sTuGVg87V92VTlkiRJ0rzQcnkJVfUgcECSXYBPJ3k68E7g+8D2wArgHcDJQIY1MU35RpIcBxwHsGTJEsbHx2fiJUxr/fr1D+nnhP03NO93mNl4vZresPmgxc05ocmcExrkfFhcmobuCVV1d5Jx4JCq+ru++IEkZwNv7x+vBfYe2Gwv4Na+fGxS+fiQPlbQhXiWLVtWY2Njk6vMuPHxcSb3c/Tyi5r3O8yaI8c2WUdtDZsPWtycE5rMOaFBzofFpeXZSx7b7+EmyaOBFwHf6ddpkyTAS4Fr+k1WAq/tz2JyEHBPVa0DPg8cnGTXJLsCB/dlkiRJ0rzQck/3HsC5SbahC/cXVNWFSb6c5LF0y0auAN7Q178YOAxYDfwQOAagqu5M8m7g0r7eyVV1Z8NxS5IkSTOqWeiuqquAZw4pf8EU9Qs4fornzgLOmtEBSpIkSbPEK1JKkiRJjRm6JUmSpMYM3ZIkSVJjhm5JkiSpMUO3JEmS1JihW5IkSWrM0C1JkiQ1ZuiWJEmSGjN0S5IkSY0ZuiVJkqTGDN2SJElSY4ZuSZIkqTFDtyRJktSYoVuSJElqzNAtSZIkNWboliRJkhozdEuSJEmNGbolSZKkxgzdkiRJUmOGbkmSJKkxQ7ckSZLUmKFbkiRJaszQLUmSJDVm6JYkSZIaM3RLkiRJjRm6JUmSpMYM3ZIkSVJjhm5JkiSpMUO3JEmS1JihW5IkSWrM0C1JkiQ1ZuiWJEmSGjN0S5IkSY0ZuiVJkqTGDN2SJElSY4ZuSZIkqTFDtyRJktSYoVuSJElqzNAtSZIkNWboliRJkhozdEuSJEmNGbolSZKkxgzdkiRJUmOGbkmSJKkxQ7ckSZLUmKFbkiRJaqxZ6E7yqCTfTHJlkmuTvKsv3yfJN5LckOTjSbbvyx/ZP17dP790oK139uXXJ3lxqzFLkiRJLbTc0/0A8IKqegZwAHBIkoOAvwFOq6p9gbuAY/v6xwJ3VdVTgNP6eiTZDzgCeBpwCPCBJNs0HLckSZI0o5qF7uqs7x9u198KeAHwib78XOCl/f3D+8f0z78wSfry86vqgaq6CVgNHNhq3JIkSdJMa7qmO8k2Sa4AbgdWAf8/cHdVbeirrAX27O/vCdwC0D9/D/CLg+VDtpEkSZK2etu2bLyqHgQOSLIL8GngqcOq9T8zxXNTlW8kyXHAcQBLlixhfHx8S4a8WdavX/+Qfk7Yf8Pwyo3NxuvV9IbNBy1uzglN5pzQIOfD4tI0dE+oqruTjAMHAbsk2bbfm70XcGtfbS2wN7A2ybbAzsCdA+UTBrcZ7GMFsAJg2bJlNTY21ubFDBgfH2dyP0cvv6h5v8OsOXJsk3XU1rD5oMXNOaHJnBMa5HxYXFqeveSx/R5ukjwaeBFwHfAV4A/7akcBn+nvr+wf0z//5aqqvvyI/uwm+wD7At9sNW5JkiRpprXc070HcG5/ppFHABdU1YVJvg2cn+SvgH8Bzuzrnwl8KMlquj3cRwBU1bVJLgC+DWwAju+XrUiSJEnzQrPQXVVXAc8cUn4jQ84+UlU/Bl4+RVvvAd4z02OUJEmSZoNXpJQkSZIaM3RLkiRJjRm6JUmSpMYM3ZIkSVJjhm5JkiSpMUO3JEmS1JihW5IkSWrM0C1JkiQ1ZuiWJEmSGjN0S5IkSY0ZuiVJkqTGDN2SJElSY4ZuSZIkqTFDtyRJktSYoVuSJElqzNAtSZIkNWboliRJkhozdEuSJEmNGbolSZKkxgzdkiRJUmOGbkmSJKkxQ7ckSZLUmKFbkiRJaszQLUmSJDVm6JYkSZIaM3RLkiRJjRm6JUmSpMYM3ZIkSVJjhm5JkiSpMUO3JEmS1JihW5IkSWrM0C1JkiQ1ZuiWJEmSGjN0S5IkSY0ZuiVJkqTGDN2SJElSY4ZuSZIkqTFDtyRJktSYoVuSJElqzNAtSZIkNWboliRJkhozdEuSJEmNGbolSZKkxgzdkiRJUmOGbkmSJKkxQ7ckSZLUmKFbkiRJaqxZ6E6yd5KvJLkuybVJ3tyXn5Tke0mu6G+HDWzzziSrk1yf5MUD5Yf0ZauTLG81ZkmSJKmFbRu2vQE4oaq+lWQn4PIkq/rnTquqvxusnGQ/4AjgacDjgS8m+eX+6fcDvw2sBS5NsrKqvt1w7JIkSdKMaRa6q2odsK6/f1+S64A9p9nkcOD8qnoAuCnJauDA/rnVVXUjQJLz+7qGbkmSJM0LLfd0/0ySpcAzgW8AzwPemOS1wGV0e8Pvogvklwxstpafh/RbJpU/Z0gfxwHHASxZsoTx8fEZfQ3DrF+//iH9nLD/hub9DjMbr1fTGzYftLg5JzSZc0KDnA+LS/PQnWRH4JPAW6rq3iRnAO8Gqv95KvA6IEM2L4avO6+HFFStAFYALFu2rMbGxmZk/NMZHx9ncj9HL7+oeb/DrDlybJN11Naw+aDFzTmhyZwTGuR8WFyahu4k29EF7o9U1acAquq2gec/CFzYP1wL7D2w+V7Arf39qcolSZKkrV7Ls5cEOBO4rqreO1C+x0C1lwHX9PdXAkckeWSSfYB9gW8ClwL7JtknyfZ0B1uubDVuSZIkaaa13NP9POA1wNVJrujL/gx4VZID6JaIrAFeD1BV1ya5gO4AyQ3A8VX1IECSNwKfB7YBzqqqaxuOW5IkSZpRLc9e8jWGr9O+eJpt3gO8Z0j5xdNtJ0mSJG3NvCKlJEmS1JihW5IkSWrM0C1JkiQ1ZuiWJEmSGjN0S5IkSY0ZuiVJkqTGDN2SJElSY4ZuSZIkqTFDtyRJktSYoVuSJElqzNAtSZIkNWboliRJkhozdEuSJEmNGbolSZKkxgzdkiRJUmOGbkmSJKkxQ7ckSZLUmKFbkiRJaszQLUmSJDVm6JYkSZIa26zQnWTXJL/aajCSJEnSQrTJ0J1kPMljkuwGXAmcneS97YcmSZIkLQyj7OneuaruBX4fOLuqng28qO2wJEmSpIVjlNC9bZI9gFcAFzYejyRJkrTgjBK63wV8HlhdVZcmeRJwQ9thSZIkSQvHttM9mWQbYO+q+tnBk1V1I/AHrQcmSZIkLRTT7umuqgeB35ulsUiSJEkL0rR7unv/nOR9wMeB+ycKq+pbzUYlSZIkLSCjhO5f63+ePFBWwAtmfjiSJEnSwrPJ0F1Vz5+NgUiSJEkL1SgXx1mS5Mwkn+sf75fk2PZDkyRJkhaGUU4ZeA7dKQMf3z/+v8BbWg1IkiRJWmhGCd27V9UFwE8BqmoD8GDTUUmSJEkLyCih+/4kv0h38CRJDgLuaToqSZIkaQEZ5ewlbwNWAk9O8n+AxwJ/2HRUkiRJ0gIyytlLvpXkt4BfAQJcX1U/aT4ySZIkaYHYZOjuLwV/GLC0r39wEqrqvY3HJkmSJC0Ioywv+SzwY+Bq+oMpJUmSJI1ulNC9V1X9avORSJIkSQvUKGcv+VySg5uPRJIkSVqgRtnTfQnw6SSPAH5CdzBlVdVjmo5MkiRJWiBGCd2nAs8Frq6qajweSZIkacEZZXnJDcA1Bm5JkiRpy4yyp3sdMJ7kc8ADE4WeMlCSJEkazSih+6b+tn1/kyRJkrQZRrki5bsAkuzUPaz1zUclSZIkLSCbXNOd5OlJ/gW4Brg2yeVJnjbCdnsn+UqS65Jcm+TNffluSVYluaH/uWtfniSnJ1md5Kokzxpo66i+/g1JjtrylytJkiTNvlEOpFwBvK2qnlhVTwROAD44wnYbgBOq6qnAQcDxSfYDlgNfqqp9gS/1jwEOBfbtb8cBZ0AX0oETgecABwInTgR1SZIkaT4YJXTvUFVfmXhQVePADpvaqKrWVdW3+vv3AdcBewKHA+f21c4FXtrfPxw4rzqXALsk2QN4MbCqqu6sqruAVcAho7w4SZIkaWswyoGUNyb5C+BD/eNX0x1YObIkS4FnAt8AllTVOuiCeZLH9dX2BG4Z2GxtXzZVuSRJkjQvjBK6Xwe8C/hU//irwNGjdpBkR+CTwFuq6t4kU1YdUlbTlE/u5zi6ZSksWbKE8fHxUYe4xdavX/+Qfk7Yf0PzfoeZjder6Q2bD1rcnBOazDmhQc6HxWWU0P2iqnrTYEGSlwN/v6kNk2xHF7g/UlUTof22JHv0e7n3AG7vy9cCew9svhdwa18+Nql8fHJfVbWCbv05y5Ytq7GxsclVZtz4+DiT+zl6+UXN+x1mzZFjm6yjtobNBy1uzglN5pzQIOfD4jLKmu53jli2kXS7tM8Erpt0IZ2VwMQZSI4CPjNQ/tr+LCYHAff0y1A+DxycZNf+AMqD+zJJkiRpXphyT3eSQ4HDgD2TnD7w1GPozkyyKc8DXgNcneSKvuzPgFOAC5IcC9wMvLx/7uK+v9XAD4FjAKrqziTvBi7t651cVXeO0L8kSZK0VZhuecmtwGXA7wGXD5TfB7x1Uw1X1dcYvh4b4IVD6hdw/BRtnQWctak+JUmSpK3RlKG7qq4Erkzy0ar6CUC/vGPv/tR9kiRJkkYwypruVUke01+k5krg7CTv3dRGkiRJkjqjhO6dq+pe4PeBs6vq2cCL2g5LkiRJWjhGCd3b9qf2ewVwYePxSJIkSQvOKKH7ZLpT9K2uqkuTPAm4oe2wJEmSpIVjkxfHqaq/Z+BCOFV1I/AHLQclSZIkLSSbDN1JzmbIZder6nVNRiRJkiQtMKNcBn5wHfejgJfRncNbkiRJ0ghGWV7yycHHST4GfLHZiCRJkqQFZpQDKSfbF3jCTA9EkiRJWqhGWdN9Hxuv6f4+8I5mI5IkSZIWmFGWl+w0GwORJEmSFqpNLi9J8rIkOw883iXJS9sOS5IkSVo4RlnTfWJV3TPxoKruBk5sNyRJkiRpYRkldA+rM8qpBiVJkiQxWui+LMl7kzw5yZOSnAZc3npgkiRJ0kIxSuj+Y+DfgI8DFwA/Ao5vOShJkiRpIRnl7CX3A8tnYSySJEnSgrQlF8eRJEmStBkM3ZIkSVJjhm5JkiSpsVEujrNXkk8n+dcktyX5ZJK9ZmNwkiRJ0kIwyp7us4GVwB7AnsBn+zJJkiRJIxgldD+2qs6uqg397RzgsY3HJUmSJC0Yo4TuO5K8Osk2/e3VwA9aD0ySJElaKEYJ3a8DXgF8H1gH/GFfJkmSJGkEo1wc52bg92ZhLJIkSdKCNGXoTvKX02xXVfXuBuORJEmSFpzp9nTfP6RsB+BY4BcBQ7ckSZI0gilDd1WdOnE/yU7Am4FjgPOBU6faTpIkSdLGpl3TnWQ34G3AkcC5wLOq6q7ZGJgkSZK0UEy3pvu/Ar8PrAD2r6r1szYqSZIkaQGZ7pSBJwCPB/4cuDXJvf3tviT3zs7wJEmSpPlvujXdo5zDW5IkSdImGKwlSZKkxgzdkiRJUmOGbkmSJKkxQ7ckSZLUmKFbkiRJaszQLUmSJDVm6JYkSZIaM3RLkiRJjRm6JUmSpMYM3ZIkSVJjhm5JkiSpMUO3JEmS1Fiz0J3krCS3J7lmoOykJN9LckV/O2zguXcmWZ3k+iQvHig/pC9bnWR5q/FKkiRJrbTc030OcMiQ8tOq6oD+djFAkv2AI4Cn9dt8IMk2SbYB3g8cCuwHvKqvK0mSJM0b27ZquKq+mmTpiNUPB86vqgeAm5KsBg7sn1tdVTcCJDm/r/vtGR6uJEmS1MxcrOl+Y5Kr+uUnu/ZlewK3DNRZ25dNVS5JkiTNG832dE/hDODdQPU/TwVeB2RI3WL4HwU1rOEkxwHHASxZsoTx8fEZGO701q9f/5B+Tth/Q/N+h5mN16vpDZsPWtycE5rMOaFBzofFZVZDd1XdNnE/yQeBC/uHa4G9B6ruBdza35+qfHLbK4AVAMuWLauxsbGZGfQ0xsfHmdzP0csvat7vMGuOHNtkHbU1bD5ocXNOaDLnhAY5HxaXWV1ekmSPgYcvAybObLISOCLJI5PsA+wLfBO4FNg3yT5Jtqc72HLlbI5ZkiRJeria7elO8jFgDNg9yVrgRGAsyQF0S0TWAK8HqKprk1xAd4DkBuD4qnqwb+eNwOeBbYCzquraVmOWJEmSWmh59pJXDSk+c5r67wHeM6T8YuDiGRyaJEmSNKu8IqUkSZLUmKFbkiRJaszQLUmSJDVm6JYkSZIaM3RLkiRJjRm6JUmSpMYM3ZIkSVJjhm5JkiSpMUO3JEmS1JihW5IkSWrM0C1JkiQ1ZuiWJEmSGjN0S5IkSY0ZuiVJkqTGDN2SJElSY4ZuSZIkqTFDtyRJktSYoVuSJElqzNAtSZIkNWboliRJkhozdEuSJEmNGbolSZKkxgzdkiRJUmOGbkmSJKkxQ7ckSZLUmKFbkiRJaszQLUmSJDVm6JYkSZIaM3RLkiRJjRm6JUmSpMYM3ZIkSVJjhm5JkiSpMUO3JEmS1JihW5IkSWrM0C1JkiQ1ZuiWJEmSGjN0S5IkSY0ZuiVJkqTGDN2SJElSY4ZuSZIkqTFDtyRJktSYoVuSJElqbNu5HoBmxtLlF81Z32tOecmc9S1JkjQfuKdbkiRJaszQLUmSJDVm6JYkSZIaaxa6k5yV5PYk1wyU7ZZkVZIb+p+79uVJcnqS1UmuSvKsgW2O6uvfkOSoVuOVJEmSWmm5p/sc4JBJZcuBL1XVvsCX+scAhwL79rfjgDOgC+nAicBzgAOBEyeCuiRJkjQw3b1cAAANBUlEQVRfNAvdVfVV4M5JxYcD5/b3zwVeOlB+XnUuAXZJsgfwYmBVVd1ZVXcBq3hokJckSZK2arO9pntJVa0D6H8+ri/fE7hloN7avmyqckmSJGne2FrO050hZTVN+UMbSI6jW5rCkiVLGB8fn7HBTWX9+vUP6eeE/Tc073drMxvv9XwwbD5ocXNOaDLnhAY5HxaX2Q7dtyXZo6rW9ctHbu/L1wJ7D9TbC7i1Lx+bVD4+rOGqWgGsAFi2bFmNjY0NqzajxsfHmdzP0XN4kZq5subIsbkewlZh2HzQ4uac0GTOCQ1yPiwus728ZCUwcQaSo4DPDJS/tj+LyUHAPf3yk88DByfZtT+A8uC+TJIkSZo3mu3pTvIxur3UuydZS3cWklOAC5IcC9wMvLyvfjFwGLAa+CFwDEBV3Znk3cClfb2Tq2rywZmSJEnSVq1Z6K6qV03x1AuH1C3g+CnaOQs4awaHJkmSJM0qr0gpSZIkNWboliRJkhozdEuSJEmNGbolSZKkxgzdkiRJUmOGbkmSJKkxQ7ckSZLUmKFbkiRJaszQLUmSJDVm6JYkSZIaM3RLkiRJjRm6JUmSpMYM3ZIkSVJjhm5JkiSpMUO3JEmS1JihW5IkSWrM0C1JkiQ1ZuiWJEmSGjN0S5IkSY0ZuiVJkqTGDN2SJElSY4ZuSZIkqTFDtyRJktSYoVuSJElqzNAtSZIkNWboliRJkhozdEuSJEmNGbolSZKkxgzdkiRJUmOGbkmSJKkxQ7ckSZLUmKFbkiRJaszQLUmSJDVm6JYkSZIaM3RLkiRJjRm6JUmSpMYM3ZIkSVJjhm5JkiSpMUO3JEmS1JihW5IkSWrM0C1JkiQ1ZuiWJEmSGjN0S5IkSY0ZuiVJkqTGDN2SJElSY4ZuSZIkqTFDtyRJktTYnITuJGuSXJ3kiiSX9WW7JVmV5Ib+5659eZKcnmR1kquSPGsuxixJkiRtqW3nsO/nV9UdA4+XA1+qqlOSLO8fvwM4FNi3vz0HOKP/qa3E0uUXzUm/a055yZz0K0mStLm2puUlhwPn9vfPBV46UH5edS4Bdkmyx1wMUJIkSdoSqarZ7zS5CbgLKOB/VtWKJHdX1S4Dde6qql2TXAicUlVf68u/BLyjqi6b1OZxwHEAS5Ysefb555/f/HWsX7+eHXfccaOyq793T/N+1dl/z53neggbGTYftLg5JzSZc0KDnA8Lw/Of//zLq2rZpurN1fKS51XVrUkeB6xK8p1p6mZI2UP+UqiqFcAKgGXLltXY2NiMDHQ64+PjTO7n6DlaarEYrTlybK6HsJFh80GLm3NCkzknNMj5sLjMyfKSqrq1/3k78GngQOC2iWUj/c/b++prgb0HNt8LuHX2RitJkiQ9PLMeupPskGSnifvAwcA1wErgqL7aUcBn+vsrgdf2ZzE5CLinqtbN8rAlSZKkLTYXy0uWAJ9OMtH/R6vqH5NcClyQ5FjgZuDlff2LgcOA1cAPgWNmf8iSJEnSlpv10F1VNwLPGFL+A+CFQ8oLOH4WhiZJkiQ1sTWdMlCSJElakAzdkiRJUmOGbkmSJKkxQ7ckSZLUmKFbkiRJaszQLUmSJDVm6JYkSZIaM3RLkiRJjRm6JUmSpMYM3ZIkSVJjhm5JkiSpMUO3JEmS1JihW5IkSWrM0C1JkiQ1ZuiWJEmSGjN0S5IkSY0ZuiVJkqTGDN2SJElSY4ZuSZIkqTFDtyRJktTYtnM9AGlLLV1+0Zz1veaUl8xZ35Ikaf5xT7ckSZLUmKFbkiRJaszQLUmSJDVm6JYkSZIaM3RLkiRJjRm6JUmSpMYM3ZIkSVJjhm5JkiSpMUO3JEmS1JihW5IkSWrMy8BLW2DYJehP2H8DRze+NL2Xn5ckaX5yT7ckSZLUmKFbkiRJaszQLUmSJDVm6JYkSZIaM3RLkiRJjRm6JUmSpMYM3ZIkSVJjhm5JkiSpMS+OI80jwy7KM1u8MI8kSVvOPd2SJElSY+7pljSSudrL7h52SdJC4J5uSZIkqTH3dEvaqrmHXZK0ELinW5IkSWps3oTuJIckuT7J6iTL53o8kiRJ0qjmxfKSJNsA7wd+G1gLXJpkZVV9e25HJmmhmollLSfsv4Gjt6Adl7ZI0sIzL0I3cCCwuqpuBEhyPnA4YOiWpBnieeAlqZ35Err3BG4ZeLwWeM4cjUWSmprL8DtXFuMBs4vxNUuL2XwJ3RlSVhtVSI4Djusfrk9yffNRwe7AHbPQj+aBNzkfNIlzYuuXv5n1Lud8TszBa9bU5nw+aEY8cZRK8yV0rwX2Hni8F3DrYIWqWgGsmM1BJbmsqpbNZp/aejkfNJlzQpM5JzTI+bC4zJezl1wK7JtknyTbA0cAK+d4TJIkSdJI5sWe7qrakOSNwOeBbYCzquraOR6WJEmSNJJ5EboBqupi4OK5Hscks7qcRVs954Mmc05oMueEBjkfFpFU1aZrSZIkSdpi82VNtyRJkjRvGbq3gJekX7ySrElydZIrklzWl+2WZFWSG/qfu/blSXJ6P0+uSvKsuR29Hq4kZyW5Pck1A2Wb/fknOaqvf0OSo+bitWhmTDEnTkryvf73xBVJDht47p39nLg+yYsHyv1/ZQFIsneSryS5Lsm1Sd7cl/t7QobuzTVwSfpDgf2AVyXZb25HpVn2/Ko6YOA0T8uBL1XVvsCX+sfQzZF9+9txwBmzPlLNtHOAQyaVbdbnn2Q34ES6C3wdCJw48R+w5qVzeOicADit/z1xQH9MEv3/FUcAT+u3+UCSbfx/ZUHZAJxQVU8FDgKO7z9Lf0/I0L0FfnZJ+qr6N2DikvRavA4Hzu3vnwu8dKD8vOpcAuySZI+5GKBmRlV9FbhzUvHmfv4vBlZV1Z1VdRewiuGhTfPAFHNiKocD51fVA1V1E7Ca7v8U/19ZIKpqXVV9q79/H3Ad3VW1/T0hQ/cWGHZJ+j3naCyafQV8Icnl/VVQAZZU1TrofuECj+vLnSuLw+Z+/s6LxeGN/XKBswb2UDonFpEkS4FnAt/A3xPC0L0lNnlJei1oz6uqZ9F9JXh8kt+cpq5zZXGb6vN3Xix8ZwBPBg4A1gGn9uXOiUUiyY7AJ4G3VNW901UdUuacWKAM3Ztvk5ek18JVVbf2P28HPk33tfBtE8tG+p+399WdK4vD5n7+zosFrqpuq6oHq+qnwAfpfk+Ac2JRSLIdXeD+SFV9qi/294QM3VvAS9IvUkl2SLLTxH3gYOAaus9/4sjyo4DP9PdXAq/tj04/CLhn4utFLSib+/l/Hjg4ya79soOD+zItEJOO3XgZ3e8J6ObEEUkemWQfuoPnvon/rywYSQKcCVxXVe8deMrfE5o/V6TcWnhJ+kVtCfDp7ncq2wIfrap/THIpcEGSY4GbgZf39S8GDqM7WOqHwDGzP2TNpCQfA8aA3ZOspTu7wClsxudfVXcmeTdd0AI4uapGPRBPW5kp5sRYkgPolgOsAV4PUFXXJrkA+DbdWS6Or6oH+3b8f2VheB7wGuDqJFf0ZX+GvyeEV6SUJEmSmnN5iSRJktSYoVuSJElqzNAtSZIkNWboliRJkhozdEuSJEmNGbolaR5IUklOHXj89iQnNehnaZI/Gni8LMnpM92PJC02hm5Jmh8eAH4/ye4Pt6Ek012jYSnws9BdVZdV1Zsebp+StNgZuiVpftgArADeOvmJJE9M8qUkV/U/nzCkzklJViT5AnBev0f7n5J8q7/9Wl/1FOA3klyR5K1JxpJcONDGWUnGk9yY5E0D7f9Fku8kWZXkY0ne3pe/Kcm3+7Gd3+B9kaR5wStSStL88X7gqiR/O6n8fcB5VXVuktcBpwMvHbL9s4Ffr6ofJfkF4Ler6sdJ9gU+BiwDlgNvr6rfAUgyNqmNfwc8H9gJuD7JGcAzgD8Ankn3/8q3gMv7+suBfarqgSS7PIzXLknzmnu6JWmeqKp7gfOAycs9ngt8tL//IeDXp2hiZVX9qL+/HfDBJFcDfw/sN+IwLqqqB6rqDuB2YEnf32eq6kdVdR/w2YH6VwEfSfJqur31krQoGbolaX75b8CxwA7T1Kkpyu8fuP9W4Da6vdTLgO1H7P+BgfsP0u3ZzjT1X0K3h/7ZwOWbWE8uSQuWoVuS5pGquhO4gC54T/hn4Ij+/pHA10ZoamdgXVX9FHgNsE1ffh/d0pHN8TXgd5M8KsmOdEGbJI8A9q6qrwB/CuwC7LiZbUvSgmDolqT551Rg8CwmbwKOSXIVXYB+8whtfAA4KsklwC/z873gVwEbklyZ5CEHbQ5TVZcCK4ErgU8BlwH30AX5D/dLWP4FOK2q7h6lTUlaaFI11beQkiSNJsmOVbW+P0Dzq8BxVfWtuR6XJG0tXFsnSZoJK5LsBzwKONfALUkbc0+3JEmS1JhruiVJkqTGDN2SJElSY4ZuSZIkqTFDtyRJktSYoVuSJElqzNAtSZIkNfb/AKuCCffuguUcAAAAAElFTkSuQmCC","text/plain":["<Figure size 864x576 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","movie_length[['user_id', 'item_id']].groupby(['user_id']).count().\\\n","hist(bins = 20, figsize = (12, 8))\n","plt.title('Distribution of no ratings by each customer')\n","plt.xlabel('No ratings')\n","plt.ylabel('No customers')"]},{"cell_type":"markdown","metadata":{"_uuid":"35a508056fb88a02419d6af847e1d28257e39d66"},"source":["Looking at the distribution of the number of users according to rating level, we can see that our sample has an imbalance phenomenon when many users rate very little and many users rate more. However, this is not a classification problem, so the fact that the sample size is imbalanced does not affect the accuracy of the algorithm. Furthermore, the `matrix factorization` algorithm builds an individual loss function for each user so that how many products this user rates does not affect the rating prediction results of other users. Completely similar, we can also statistics the number of user ratings for each movie."]},{"cell_type":"code","execution_count":5,"metadata":{"_uuid":"a06ceb5c3faf499214a775e260b5a6f99c28e019","execution":{"iopub.execute_input":"2024-04-25T15:57:29.034547Z","iopub.status.busy":"2024-04-25T15:57:29.033986Z","iopub.status.idle":"2024-04-25T15:57:29.072549Z","shell.execute_reply":"2024-04-25T15:57:29.072063Z","shell.execute_reply.started":"2024-04-25T15:57:29.034480Z"},"trusted":true},"outputs":[{"data":{"text/plain":["count    3706.000000\n","mean      269.888829\n","std       384.046815\n","min         1.000000\n","25%        33.000000\n","50%       123.500000\n","75%       350.000000\n","max      3428.000000\n","Name: item_id, dtype: float64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["movie_length['item_id'].value_counts().describe()"]},{"cell_type":"code","execution_count":6,"metadata":{"_uuid":"cf2bac8083738a8d13097e7ee5a9f8d933071876","execution":{"iopub.execute_input":"2024-04-25T15:57:29.073612Z","iopub.status.busy":"2024-04-25T15:57:29.073324Z","iopub.status.idle":"2024-04-25T15:57:29.447332Z","shell.execute_reply":"2024-04-25T15:57:29.446357Z","shell.execute_reply.started":"2024-04-25T15:57:29.073577Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Text(0,0.5,'No movies')"]},"execution_count":6,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 864x576 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["movie_length[['user_id', 'item_id']].groupby(['item_id']).count().\\\n","hist(bins = 20, figsize = (12, 8))\n","plt.title('Distribution of no ratings per each movie')\n","plt.xlabel('No ratings')\n","plt.ylabel('No movies')"]},{"cell_type":"markdown","metadata":{"_uuid":"ceeadb9c4ad2f7ed00e5d5258af2abfbf35ba833"},"source":["A few reviews:\n","* The number of times a movie is rated at least once.\n","* The highest number of times a movie has been rated is 3,428 times.\n","* The popular rating of a movie is from 33 to 123 times.\n","\n","## 2.2. Matrix factorization algorithm\n","\n","### 2.2.1. Functions in algorithms\n","\n","First, we will divide the train and test sample proportionally so that the number of ratings in the train set accounts for 2/3 of the number of ratings. The most reasonable way to divide the sample is to ensure that the ratio of the number of ratings appearing in the training set to the number of ratings appearing in the test set of the same user is equal. This division ensures fairness for users when no user has too much training data and little test data or too little training data but too much test data. The predicted value from a model with too little training data will often be inaccurate and distort the error checking results on the test."]},{"cell_type":"code","execution_count":7,"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"a31e9751b6eb95dc8a0cca52b59b4c5e1ae2b7ff","execution":{"iopub.execute_input":"2024-04-25T15:57:29.449509Z","iopub.status.busy":"2024-04-25T15:57:29.448940Z","iopub.status.idle":"2024-04-25T15:57:34.748584Z","shell.execute_reply":"2024-04-25T15:57:34.747729Z","shell.execute_reply.started":"2024-04-25T15:57:29.449443Z"},"trusted":true},"outputs":[],"source":["#declare split_rate for train/total ratings\n","split_rate = 2/3\n","\n","def split_train_test(dataset):\n","    gb = dataset.groupby('user_id')\n","    ls = [gb.get_group(x) for x in gb.groups]\n","    items = [x for x in gb.groups]\n","    index_size = [{'i': i, 'index':gb.groups[i], 'size':len(gb.groups[i])} for i in items]\n","    index_train = pd.Int64Index([])\n","    index_test = pd.Int64Index([])\n","    for x in index_size:\n","        np.random.shuffle(x['index'].values)\n","        le = int(x['size']*split_rate)\n","        index_train = index_train.append(x['index'][:le])\n","        index_test = index_test.append(x['index'][le:])\n","    train = dataset.iloc[index_train].values\n","    test = dataset.iloc[index_test].values\n","    #minus id to 1 to index start from 0\n","    train[:, 0] -= 1\n","    train[:, 1] -= 1\n","    test[:, 0] -= 1\n","    test[:, 1] -= 1\n","    return train, test\n","\n","train, test = split_train_test(movie_length)"]},{"cell_type":"markdown","metadata":{"_uuid":"5b9845f4338cc598ade78720a84f13d238c5fb7f"},"source":["**Proceed to build the algorithm:**\n","\n","Key variables for the data reporting algorithm include:\n","* n_users: Number of users (which is $N$ in the algorithm).\n","* n_items: Number of items (which is $M$ in the algorithm).\n","* K: Number of hidden factors used (value $K$ in the algorithm).\n","* theta: Parameter $\\theta$ to update coefficients in the gradient descent algorithm.\n","* split_rate: Train/test sample split ratio.\n","* lamda: Calibration parameter of the `l2 - regularization` component (For simplicity set $\\lambda_1 = \\lambda_2$).\n","* I: Product matrix.\n","* U: User matrix.\n","\n","Note that the matrices $\\mathbf{I}, \\mathbf{U}$ are built based on *hidden factors* (latent features), so initially we cannot determine these factors and must initialize them. random value for them. The number of hidden factors $K$ is an arbitrary value that we can choose. According to [Matrix Factorization For Recommendation System](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf), the more the number of hidden factors, the more accurate the algorithm is but also increases computational costs. At the same time, models built on hidden factors that have been refined to have a large degree of variance also provide more accurate results than creating random hidden factors."]},{"cell_type":"code","execution_count":8,"metadata":{"_uuid":"2f66f994447184ed436aa39e6dc03a59dd453617","execution":{"iopub.execute_input":"2024-04-25T15:57:34.750110Z","iopub.status.busy":"2024-04-25T15:57:34.749832Z","iopub.status.idle":"2024-04-25T15:57:34.759080Z","shell.execute_reply":"2024-04-25T15:57:34.758373Z","shell.execute_reply.started":"2024-04-25T15:57:34.750058Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["N user dimesion: 6040\n","M item dimesion: 3952\n","S Number of rating: 664826\n"]}],"source":["n_users = np.max(train[:, 0] + 1) #plus one because index start from 0\n","n_items = np.max(train[:, 1] + 1)\n","n_ratings = train.shape[0]\n","print('N user dimesion: %s'%n_users)\n","print('M item dimesion: %s'%n_items)\n","print('S Number of rating: %s'%n_ratings)\n","K = 2\n","theta = 0.75\n","lamda = 0.2\n","#Inititalize random matrix according to Gauss distribution\n","I = np.random.randn(n_items, K)\n","U = np.random.randn(K, n_users)"]},{"cell_type":"code","execution_count":9,"metadata":{"_uuid":"a15ba1fcd16770a85d7ffd123268889d098c484e","execution":{"iopub.execute_input":"2024-04-25T15:57:34.760551Z","iopub.status.busy":"2024-04-25T15:57:34.760234Z","iopub.status.idle":"2024-04-25T15:57:34.967515Z","shell.execute_reply":"2024-04-25T15:57:34.966922Z","shell.execute_reply.started":"2024-04-25T15:57:34.760479Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Y utility matrix shape: (3952, 6040)\n"]}],"source":["import scipy.sparse as sparse\n","#Rating matrix\n","Y = np.zeros(shape = (n_items, n_users))\n","print('Y utility matrix shape: %s'%str(Y.shape))\n","Y = sparse.coo_matrix((train[:, 2], (train[:, 1], train[:, 0])),\\\n","                      shape = (n_items, n_users), dtype = np.float).toarray()"]},{"cell_type":"markdown","metadata":{"_uuid":"b22a47980d6fcdc27fdf4c96a281d1aba5f02abb"},"source":["Not all values on the matrix $\\mathbf{Y}$ are rated. Therefore, the matrix $\\mathbf{R}$ is created to mark the rated positions of $\\mathbf{Y}$ with value 1 and not yet rated with 0."]},{"cell_type":"code","execution_count":10,"metadata":{"_uuid":"fa4d5d61e0956f8694065d4030b58dd55123f7c1","execution":{"iopub.execute_input":"2024-04-25T15:57:34.969157Z","iopub.status.busy":"2024-04-25T15:57:34.968769Z","iopub.status.idle":"2024-04-25T15:57:35.164089Z","shell.execute_reply":"2024-04-25T15:57:35.163484Z","shell.execute_reply.started":"2024-04-25T15:57:34.969092Z"},"trusted":true},"outputs":[],"source":["R = sparse.coo_matrix((np.ones((n_ratings,)), (train[:, 1], train[:, 0])),\\\n","                      shape = (n_items, n_users)).toarray()"]},{"cell_type":"markdown","metadata":{"_uuid":"94487acb57ab4b68bf3c3e1e9f2222b90724002b"},"source":["In order for the `gradient descent` algorithm to converge faster, we need to normalize the matrix $\\mathbf{Y}$ to an expected value of 0 by subtracting each rating value in a user's rating vector from the average of each user's rating vector. that rating vector."]},{"cell_type":"code","execution_count":11,"metadata":{"_uuid":"9ceb8b4521a168218eb0cafd99f2285b87f9d2a9","execution":{"iopub.execute_input":"2024-04-25T15:57:35.165592Z","iopub.status.busy":"2024-04-25T15:57:35.165378Z","iopub.status.idle":"2024-04-25T15:57:50.071433Z","shell.execute_reply":"2024-04-25T15:57:50.070829Z","shell.execute_reply.started":"2024-04-25T15:57:35.165554Z"},"trusted":true},"outputs":[],"source":["def standardize_Y(Y):\n","    sum_rating = Y.sum(axis = 0)\n","    u_rating = np.count_nonzero(Y, axis = 0)\n","    u_mean = sum_rating/u_rating\n","    for n in range(n_users):\n","        for m in range(n_items):\n","            if Y[m, n] != 0:\n","                Y[m, n] -= u_mean[n]\n","    return Y, u_mean\n","\n","Y_stad, u_mean = standardize_Y(Y)"]},{"cell_type":"markdown","metadata":{"_uuid":"92f444fb7af7d1731b5a3908ed3b6a48537d4125"},"source":["After normalizing the matrix $\\mathbf{Y}$, the most important part is applying the `gradient descent` algorithm to optimize the coefficients of the matrices $\\mathbf{U}, \\mathbf{I}$ . Based on the algorithm theory developed in section **1.4** to build matrix update functions:\n","\n","**Gradient descent algorithm for user matrix:**"]},{"cell_type":"code","execution_count":12,"metadata":{"_uuid":"fefd8dd0084de36ca676fb02099604243867ab84","execution":{"iopub.execute_input":"2024-04-25T15:57:50.073234Z","iopub.status.busy":"2024-04-25T15:57:50.072935Z","iopub.status.idle":"2024-04-25T15:57:50.079273Z","shell.execute_reply":"2024-04-25T15:57:50.078371Z","shell.execute_reply.started":"2024-04-25T15:57:50.073178Z"},"trusted":true},"outputs":[],"source":["def updateU(U):\n","    for n in range(n_users):\n","    # Matrix items include all items is rated by user n\n","        i_rated = np.where(Y_stad[:, n] != 0)[0] #item's index rated by n\n","        In = I[i_rated, :]\n","        if In.shape[0] == 0:\n","            U[:, n] = 0\n","        else: \n","            s = In.shape[0]\n","            u_n = U[:, n]\n","            y_n = Y_stad[i_rated, n]\n","            grad = -1/s * np.dot(In.T,(y_n-np.dot(In, u_n))) + lamda*u_n\n","            U[:, n] -= theta*grad\n","    return U"]},{"cell_type":"markdown","metadata":{"_uuid":"da1071749f9d1561b2f6bfdce57739ab17d8f3a2"},"source":["**Gradient descent algorithm for product matrix:**"]},{"cell_type":"code","execution_count":13,"metadata":{"_uuid":"85620140574c17047702680a097034e510a2aca7","execution":{"iopub.execute_input":"2024-04-25T15:57:50.080689Z","iopub.status.busy":"2024-04-25T15:57:50.080488Z","iopub.status.idle":"2024-04-25T15:57:50.091182Z","shell.execute_reply":"2024-04-25T15:57:50.090564Z","shell.execute_reply.started":"2024-04-25T15:57:50.080652Z"},"trusted":true},"outputs":[],"source":["def updateI(I):\n","    for m in range(n_items):\n","    # Matrix users who rated into item m\n","        i_rated = np.where(Y_stad[m, :] != 0)[0] #user's index rated into m\n","        Um = U[:, i_rated]\n","        if Um.shape[1] == 0: \n","            I[m, :] = 0\n","        else:\n","            s = Um.shape[1]\n","            i_m = I[m, :]\n","            y_m = Y_stad[m, i_rated]\n","            grad = -1/s * np.dot(y_m - np.dot(i_m, Um), Um.T) + lamda*i_m\n","            I[m, :] -= theta*grad\n","    return I"]},{"cell_type":"markdown","metadata":{"_uuid":"2d9e2239925583b9c42f35041aaf7efb182a459b"},"source":["**Build the matrix prediction function $\\mathbf{Y}$:**\n","\n","Based on the matrix $\\mathbf{U}$ and $\\mathbf{I}$, we can calculate the prediction matrix of $\\mathbf{Y}$ as $\\mathbf{\\hat{Y}}$ according to the formula formula *(1.4.0)* and build the *pred()* function. The forecast results need to be converted back to the rating by adding the average rating of each user to the rating values belonging to the same user. Some results will exceed the rating value range of [1, 5] and will then be reassigned to the two endpoints 1 or 5. The resulting matrix will satisfy a cell of $\\mathbf{\\hat {Y}}$ is the user rating prediction result for the corresponding product. Because we only need to evaluate the match $\\mathbf{Y}$ on pairs (user, item) that have been rated, in the function *pred_train_test()* we need to rely on the rating matrix $\\mathbf{R}$ instead unrated positions are equal to 0. The reason this function is named *pred_train_test()* is that we can do the same for the test set by replacing the unrated value with 0."]},{"cell_type":"code","execution_count":14,"metadata":{"_uuid":"60b827e6f6a5806eef8d42e10ec7ed77c8109d78","execution":{"iopub.execute_input":"2024-04-25T15:57:50.092476Z","iopub.status.busy":"2024-04-25T15:57:50.092286Z","iopub.status.idle":"2024-04-25T15:57:50.105377Z","shell.execute_reply":"2024-04-25T15:57:50.104734Z","shell.execute_reply.started":"2024-04-25T15:57:50.092441Z"},"trusted":true},"outputs":[],"source":["def pred(U, I):\n","    #predict utility matrix base on formula Y_hat = I.U\n","    Y_hat = np.dot(I, U)\n","    #invert to forecast values by plus user's mean ratings\n","    for n in range(n_users):\n","        Y_hat[:, n] += u_mean[n]\n","    #convert to interger values because of rating is integer\n","    Y_hat = Y_hat.astype(np.int32) \n","    #replace values > 5 by 5 and values < 1 by 1\n","    Y_hat[Y_hat > 5] = 5\n","    Y_hat[Y_hat < 1] = 1\n","    return Y_hat\n","\n","def pred_train_test(Y_hat, R):\n","    #replace values have not yet rated by 0 \n","    Y_pred = Y_hat.copy()\n","    Y_pred[R == 0] = 0\n","    return Y_pred"]},{"cell_type":"markdown","metadata":{"_uuid":"d69b96bda47e4da8427d7f1cd2695845470f4993"},"source":["**Build the loss function:**\n","\n","The loss function is built based on the formula *(1.4.1)* as follows:"]},{"cell_type":"code","execution_count":15,"metadata":{"_uuid":"136a291e708b9fee48f322bc6d106895141fddd4","execution":{"iopub.execute_input":"2024-04-25T15:57:50.106595Z","iopub.status.busy":"2024-04-25T15:57:50.106393Z","iopub.status.idle":"2024-04-25T15:57:50.115973Z","shell.execute_reply":"2024-04-25T15:57:50.115364Z","shell.execute_reply.started":"2024-04-25T15:57:50.106559Z"},"trusted":true},"outputs":[],"source":["def loss(Y, Y_hat):\n","    error = Y-Y_hat\n","    loss_value = 1/(2*n_ratings)*np.linalg.norm(error, 'fro')**2 + \\\n","    lamda/2*(np.linalg.norm(I, 'fro')**2 + np.linalg.norm(U, 'fro')**2)\n","    return loss_value"]},{"cell_type":"markdown","metadata":{"_uuid":"a8ac847205c7397611e3d953bce389684c15ff52"},"source":["Use the matrix $\\mathbf{\\hat{Y}}$ to predict on the test set"]},{"cell_type":"code","execution_count":16,"metadata":{"_uuid":"c2cbc416570fbcf8e1b19a8bd3d628a805e42c7c","execution":{"iopub.execute_input":"2024-04-25T15:57:50.117477Z","iopub.status.busy":"2024-04-25T15:57:50.117243Z","iopub.status.idle":"2024-04-25T15:57:50.465718Z","shell.execute_reply":"2024-04-25T15:57:50.464921Z","shell.execute_reply.started":"2024-04-25T15:57:50.117423Z"},"trusted":true},"outputs":[],"source":["Y_test = sparse.coo_matrix((test[:, 2], (test[:, 1], test[:, 0])), \\\n","                           shape = (n_items, n_users), dtype = np.float).toarray()\n","R_test = sparse.coo_matrix((np.ones(test.shape[0]), (test[:, 1], test[:, 0])), \\\n","                           shape = (n_items, n_users), dtype = np.float).toarray()"]},{"cell_type":"markdown","metadata":{"_uuid":"81c5ba76e546323ad0aae9b5c481fc96d3bc4e7e"},"source":["**Build a function to calculate RMSE:**\n","\n","After calculating the prediction matrix $\\mathbf{\\hat{Y}}$ on the test set combined with the utility matrix $\\mathbf{Y}$ of the known test set, we will calculate *RMSE* on the test set. test as follows:"]},{"cell_type":"code","execution_count":17,"metadata":{"_uuid":"3ad158b177d7ffe17d75156b1fff63a2064bc770","execution":{"iopub.execute_input":"2024-04-25T15:57:50.467296Z","iopub.status.busy":"2024-04-25T15:57:50.467043Z","iopub.status.idle":"2024-04-25T15:57:50.472032Z","shell.execute_reply":"2024-04-25T15:57:50.471073Z","shell.execute_reply.started":"2024-04-25T15:57:50.467247Z"},"trusted":true},"outputs":[],"source":["import math\n","def RMSE(Y_test, Y_pred):\n","    error = Y_test - Y_pred\n","    n_ratings = test.shape[0]\n","    rmse = math.sqrt(np.linalg.norm(error, 'fro')**2/n_ratings)\n","    return rmse"]},{"cell_type":"markdown","metadata":{"_uuid":"3619b57a607c44c915a363ca781cbdcb7c3be230"},"source":["**Building the main optimization loop:**\n","    \n","After designing the calculation functions *loss function, RMSE* and optimization functions *gradient descent*, we will proceed to build an optimization loop to update the matrices $\\mathbf{U}$ and $ \\mathbf{I}$ and evaluate the effectiveness of each iteration through the value of *loss function* and *RMSE*."]},{"cell_type":"code","execution_count":18,"metadata":{"_uuid":"89b371ae82e3e3b2a0b0060a26c3ebd12d0eff97","execution":{"iopub.execute_input":"2024-04-25T15:57:50.473531Z","iopub.status.busy":"2024-04-25T15:57:50.473252Z","iopub.status.idle":"2024-04-25T15:57:50.481261Z","shell.execute_reply":"2024-04-25T15:57:50.480664Z","shell.execute_reply.started":"2024-04-25T15:57:50.473472Z"},"trusted":true},"outputs":[],"source":["def fit(Umatrix, Imatrix, Ytrain, Ytest, n_iter, log_iter):\n","    for i in range(n_iter):\n","        #update U and I\n","        Umatrix = updateU(Umatrix)\n","        Imatrix = updateI(Imatrix)\n","        #calculate Y_hat\n","        Y_hat = pred(Umatrix, Imatrix)\n","        #calculate Y_hat_train by replace non ratings by 0\n","        Y_pred_train = pred_train_test(Y_hat, R)\n","        #calculate loss function\n","        loss_value = loss(Ytrain, Y_pred_train)\n","        #calculate Y_pred on test dataset\n","        Y_pred_test = pred_train_test(Y_hat, R_test)\n","        #calculate RMSE\n","        rmse = RMSE(Ytest, Y_pred_test)\n","        if i % log_iter == 0:\n","            print('Iteration: {}; RMSE: {}; Loss value: {}'.format(i, rmse, loss_value))\n","    return Y_hat, Y_pred_test   \n","# Y_hat, Y_pred = fit(Umatrix = U, Imatrix = I, Ytrain = Y, Ytest = Y_test, n_iter = 100, log_iter = 10)"]},{"cell_type":"markdown","metadata":{"_uuid":"e2346294dd2ab7dfa178a7754ad82b8d073953d2"},"source":["### 2.2.2. Build class MF\n","\n","Based on the functions processed in *(2.2.1)*, we will design the MF class with the function of processing data, fitting the model, evaluating model results and giving recommendations to customers about the product as follows :\n","\n","**Class Data handles data:**"]},{"cell_type":"code","execution_count":19,"metadata":{"_uuid":"e2e7295c7fe6fd120dfaeb3c7b294b5256f5e38c","execution":{"iopub.execute_input":"2024-04-25T15:57:50.482777Z","iopub.status.busy":"2024-04-25T15:57:50.482522Z","iopub.status.idle":"2024-04-25T15:57:50.498566Z","shell.execute_reply":"2024-04-25T15:57:50.497730Z","shell.execute_reply.started":"2024-04-25T15:57:50.482709Z"},"trusted":true},"outputs":[],"source":["class Data(object):\n","    \"\"\"\n","    This class used to manage data.\n","    Two arguments:\n","    dataset: pandas data frame include user_id, item_id and rating\n","    split_rate: number train ratings/ total ratings\n","    \"\"\"\n","    def __init__(self, dataset, split_rate):\n","        self.dataset = dataset\n","        self.split_rate = split_rate\n","        self.train, self.test = self.split_train_test(self.dataset)\n","        self.n_users = np.max(self.train[:, 0] + 1) #plus one because index start from 0\n","        self.n_items = np.max(self.train[:, 1] + 1)\n","        self.Ytrain, self.Rtrain = self.utility_matrix(self.train)\n","        self.Ytest , self.Rtest  = self.utility_matrix(self.test)\n","        self.Ystad,  self.u_mean = self.standardize_Y(self.Ytrain)\n","        self.n_ratings = self.train.shape[0]\n","        \n","    def split_train_test(self, dataset):\n","        \"split train and test\"\n","        gb = dataset.groupby('user_id')\n","        ls = [gb.get_group(x) for x in gb.groups]\n","        items = [x for x in gb.groups]\n","        index_size = [{'i': i, 'index':gb.groups[i], 'size':len(gb.groups[i])} for i in items]\n","        index_train = pd.Int64Index([])\n","        index_test = pd.Int64Index([])\n","        for x in index_size:\n","            np.random.shuffle(x['index'].values)\n","            le = int(x['size']*self.split_rate)\n","            index_train = index_train.append(x['index'][:le])\n","            index_test = index_test.append(x['index'][le:])\n","        train = dataset.iloc[index_train].values\n","        test = dataset.iloc[index_test].values\n","        #minus id to 1 to index start from 0\n","        train[:, 0] -= 1\n","        train[:, 1] -= 1\n","        test[:, 0] -= 1\n","        test[:, 1] -= 1\n","        return train, test\n","    \n","    def utility_matrix(self, data_mtx):\n","        \"create Y and R matrix\"\n","        Y = np.zeros(shape = (self.n_items, self.n_users))\n","        Y = sparse.coo_matrix((data_mtx[:, 2], (data_mtx[:, 1], data_mtx[:, 0])), \\\n","                              shape = (self.n_items, self.n_users), dtype = np.float).toarray()\n","        R = sparse.coo_matrix((np.ones((data_mtx.shape[0],)), (data_mtx[:, 1], data_mtx[:, 0])), \\\n","                              shape = (self.n_items, self.n_users)).toarray()\n","        return Y, R\n","    \n","    def standardize_Y(self, Y):\n","        \"standard data to mean ratings of each user = 0\"\n","        sum_rating = Y.sum(axis = 0)\n","        u_rating = np.count_nonzero(Y, axis = 0)\n","        u_mean = sum_rating/u_rating\n","        for n in range(self.n_users):\n","            for m in range(self.n_items):\n","                if Y[m, n] != 0:\n","                    Y[m, n] -= u_mean[n]\n","        return Y, u_mean"]},{"cell_type":"markdown","metadata":{"_uuid":"1e0c58d10623d4111494e33fd38fd28c8a213056"},"source":["**Class model for construction and model evaluation:**"]},{"cell_type":"code","execution_count":20,"metadata":{"_uuid":"2072581668d18c4cbbb8f4a5939c30c9c1ddcee5","execution":{"iopub.execute_input":"2024-04-25T15:57:50.500219Z","iopub.status.busy":"2024-04-25T15:57:50.499937Z","iopub.status.idle":"2024-04-25T15:57:50.516347Z","shell.execute_reply":"2024-04-25T15:57:50.515533Z","shell.execute_reply.started":"2024-04-25T15:57:50.500166Z"},"trusted":true},"outputs":[],"source":["class Model():\n","    \"\"\"\n","    This class manage update U and I matrix, predict and evaluate error\n","    Four arguments:\n","    data: instance from Data class which supplies the data for model\n","    theta: learning rate\n","    lamda: regularization parameter\n","    K: number of latent factors\n","    \"\"\"\n","    def __init__(self, data, theta, lamda, K):\n","        self.data = data\n","        self.theta = theta\n","        self.lamda = lamda\n","        self.K = K\n","        self.I = np.random.randn(data.n_items, K)\n","        self.U = np.random.randn(K, data.n_users)\n","        \n","               \n","    def updateU(self):\n","        for n in range(self.data.n_users):\n","        # Matrix items include all items is rated by user n\n","            i_rated = np.where(self.data.Ystad[:, n] != 0)[0] #item's index rated by n\n","            In = self.I[i_rated, :]\n","            if In.shape[0] == 0:\n","                self.U[:, n] = 0\n","            else: \n","                s = In.shape[0]\n","                u_n = self.U[:, n]\n","                y_n = self.data.Ystad[i_rated, n]\n","                grad = -1/s * np.dot(In.T,(y_n-np.dot(In, u_n))) + self.lamda*u_n\n","                self.U[:, n] -= self.theta*grad\n","         \n","    def updateI(self):\n","        for m in range(self.data.n_items):\n","        # Matrix users who rated into item m\n","            i_rated = np.where(self.data.Ystad[m, :] != 0)[0] #user's index rated into m\n","            Um = self.U[:, i_rated]\n","            if Um.shape[1] == 0: \n","                self.I[m, :] = 0\n","            else:\n","                s = Um.shape[1]\n","                i_m = self.I[m, :]\n","                y_m = self.data.Ystad[m, i_rated]\n","                grad = -1/s * np.dot(y_m - np.dot(i_m, Um), Um.T) + self.lamda*i_m\n","                self.I[m, :] -= self.theta*grad\n","    \n","    def pred(self, I, U):\n","        #predict utility matrix base on formula Yhat = I.U\n","        Yhat = np.dot(I, U)\n","        #invert to forecast values by plus user's mean ratings\n","        for n in range(self.data.n_users):\n","            Yhat[:, n] += self.data.u_mean[n]\n","        #convert to interger values because of rating is integer\n","        Yhat = Yhat.astype(np.int32) \n","        #replace values > 5 by 5 and values < 1 by 1\n","        Yhat[Yhat > 5] = 5\n","        Yhat[Yhat < 1] = 1\n","        return Yhat\n","\n","    def pred_train_test(self, Yhat, R):\n","        #replace values have not yet rated by 0 \n","        Y_pred = Yhat.copy()\n","        Y_pred[R == 0] = 0\n","        return Y_pred\n","    \n","    def loss(self, Y, Yhat):\n","        error = Y-Yhat\n","        n_ratings = np.sum(Y != 0)\n","        loss_value = 1/(2*n_ratings)*np.linalg.norm(error, 'fro')**2 +\\\n","        self.lamda/2*(np.linalg.norm(self.I, 'fro')**2 + \\\n","                 np.linalg.norm(self.U, 'fro')**2)\n","        return loss_value\n","    \n","    def RMSE(self, Y, Yhat):\n","        error = Y - Yhat\n","        n_ratings = np.sum(Y != 0)\n","        rmse = math.sqrt(np.linalg.norm(error, 'fro')**2/n_ratings)\n","        return rmse"]},{"cell_type":"markdown","metadata":{"_uuid":"2ef8ce8bee68b51be161f573d766b5e0ce00c541"},"source":["**Build MF class to manage model and data:**"]},{"cell_type":"code","execution_count":21,"metadata":{"_uuid":"aa9db1a1a40d0f0579b7b6d803539bef71336f42","execution":{"iopub.execute_input":"2024-04-25T15:57:50.518053Z","iopub.status.busy":"2024-04-25T15:57:50.517707Z","iopub.status.idle":"2024-04-25T15:57:50.530066Z","shell.execute_reply":"2024-04-25T15:57:50.529481Z","shell.execute_reply.started":"2024-04-25T15:57:50.517991Z"},"trusted":true},"outputs":[],"source":["class MF():\n","    \"\"\"\n","    This class used to manage model and data\n","    Two main arguments:\n","    data: control the data\n","    model: control the functions which execute model\n","    \"\"\"\n","    def __init__(self, data, model, n_iter, print_log_iter):\n","        self.data = data\n","        self.model = model\n","        self.n_iter = n_iter\n","        self.print_log_iter = print_log_iter\n","        self.Y_pred_train = None\n","        self.Y_pred_test = None\n","        self.Yhat = None\n","        \n","    def fit(self):\n","        for i in range(self.n_iter):\n","            #update U and I\n","            self.model.updateU()\n","            self.model.updateI()\n","            #calculate Y_hat\n","            self.Yhat = self.model.pred(self.model.I, self.model.U)\n","            #calculate Y_pred_train by replace non ratings by 0\n","            self.Y_pred_train = self.model.pred_train_test(self.Yhat, self.data.Rtrain)\n","            self.Y_pred_test  = self.model.pred_train_test(self.Yhat, self.data.Rtest)\n","            if i % self.print_log_iter == 0:\n","                print('Iteration: {}; RMSE: {}; Loss value: {}'.\\\n","                      format(i, self.model.RMSE(self.data.Ytest, self.Y_pred_test),\\\n","                             self.model.loss(self.data.Ytrain, self.Y_pred_train)))\n","    \n","    def RMSE(self) -> float:\n","        return self.model.RMSE(self.data.Ytest, self.Y_pred_test)\n","                \n","    def recommend_for_user(self, user_id, k_neighbors):\n","        recm = np.concatenate((np.arange(1, self.Y_pred_test.shape[0]+1).reshape(-1, 1), \\\n","                               self.Y_pred_test[:, user_id - 1].reshape(-1, 1)), axis = 1)\n","        recm.sort(axis = 0)\n","        print('Top %s item_id recommended to user_id %s: %s'%\\\n","              (k_neighbors, user_id, str(recm[-k_neighbors:, 0])))"]},{"cell_type":"code","execution_count":22,"metadata":{"_uuid":"eebcd873f7d5a129fbc41b13b16452d3e60c4690","execution":{"iopub.execute_input":"2024-04-25T15:57:50.531464Z","iopub.status.busy":"2024-04-25T15:57:50.531170Z","iopub.status.idle":"2024-04-25T15:58:11.337877Z","shell.execute_reply":"2024-04-25T15:58:11.337257Z","shell.execute_reply.started":"2024-04-25T15:57:50.531420Z"},"trusted":true},"outputs":[],"source":["data = Data(dataset = movie_length, split_rate = 2/3)\n","# model = Model(data = data, theta = 0.75, lamda = 0.1, K = 3)\n","# mf = MF(data = data, model = model, n_iter = 100, print_log_iter = 10)\n","# mf.fit()"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T15:58:11.339323Z","iopub.status.busy":"2024-04-25T15:58:11.339108Z","iopub.status.idle":"2024-04-25T16:32:32.172737Z","shell.execute_reply":"2024-04-25T16:32:32.171870Z","shell.execute_reply.started":"2024-04-25T15:58:11.339285Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration: 0; RMSE: 1.5887381626049948; Loss value: 1075.5644632490303\n","Iteration: 40; RMSE: 1.1856806970074347; Loss value: 183.29120387468907\n","Iteration: 80; RMSE: 1.100289699307377; Loss value: 280.44402967786834\n","Iteration: 120; RMSE: 1.0832796045177748; Loss value: 302.91524947998903\n","Iteration: 160; RMSE: 1.0802117819403942; Loss value: 311.013623296358\n","Iteration: 200; RMSE: 1.0756698723889921; Loss value: 322.48971448573985\n","Iteration: 240; RMSE: 1.071602701275387; Loss value: 334.11075407974596\n","Iteration: 280; RMSE: 1.068619943666933; Loss value: 342.95113424819925\n","Iteration: 320; RMSE: 1.066685972835165; Loss value: 349.25618495902336\n","Iteration: 360; RMSE: 1.0653433986281953; Loss value: 354.05865251724845\n","Model(K=3) fitting finished!\n","Iteration: 0; RMSE: 1.7237013472836238; Loss value: 1791.3189058708574\n","Iteration: 40; RMSE: 1.1951535893516976; Loss value: 273.4893239385973\n","Iteration: 80; RMSE: 1.1300088713707217; Loss value: 312.02051051063086\n","Iteration: 120; RMSE: 1.0852307118124453; Loss value: 354.4136821308738\n","Iteration: 160; RMSE: 1.0814269966992125; Loss value: 359.64192433568695\n","Iteration: 200; RMSE: 1.0769594298040082; Loss value: 371.1940093678479\n","Iteration: 240; RMSE: 1.0719281978022581; Loss value: 387.00814731432024\n","Iteration: 280; RMSE: 1.0668830211254716; Loss value: 401.88245443416406\n","Iteration: 320; RMSE: 1.06341328248412; Loss value: 413.5258806700566\n","Iteration: 360; RMSE: 1.0608516640737429; Loss value: 422.36912860298946\n","Model(K=5) fitting finished!\n","Iteration: 0; RMSE: 1.907437260909552; Loss value: 3584.6719445993035\n","Iteration: 40; RMSE: 1.186514039332964; Loss value: 553.145373055613\n","Iteration: 80; RMSE: 1.0973387262311431; Loss value: 493.5397697178334\n","Iteration: 120; RMSE: 1.0845298722491699; Loss value: 458.5544243660593\n","Iteration: 160; RMSE: 1.0805222675327348; Loss value: 450.1891466950935\n","Iteration: 200; RMSE: 1.0748851317820203; Loss value: 458.16814827791876\n","Iteration: 240; RMSE: 1.068657610845339; Loss value: 473.02192778193336\n","Iteration: 280; RMSE: 1.0638155620133651; Loss value: 488.0209574115822\n","Iteration: 320; RMSE: 1.0605860251122428; Loss value: 501.18587133566115\n","Iteration: 360; RMSE: 1.0578230898260528; Loss value: 512.4521305627801\n","Model(K=10) fitting finished!\n"]}],"source":["K_values = [3, 5, 10]\n","models = []\n","mfs = []\n","\n","for K in K_values:\n","    model = Model(data=data, theta=0.15, lamda=0.1, K=K)\n","    models.append(model)\n","\n","    \n","for model in models:\n","    mfs.append(MF(data=data, model=model, n_iter=400, print_log_iter=40))\n","\n","for mf in mfs:\n","    mf.fit()\n","    print(f\"Model(K={mf.model.K}) fitting finished!\")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T16:32:32.174412Z","iopub.status.busy":"2024-04-25T16:32:32.174104Z","iopub.status.idle":"2024-04-25T16:32:32.893811Z","shell.execute_reply":"2024-04-25T16:32:32.892655Z","shell.execute_reply.started":"2024-04-25T16:32:32.174352Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model(K=3) RMSE : 1.0639388785776431\n","Model(K=5) RMSE : 1.059109039855794\n","Model(K=10) RMSE : 1.0559978231042166\n"]}],"source":["for i in range(len(K_values)):\n","    print(f\"Model(K={models[i].K}) RMSE : {mfs[i].RMSE()}\")"]},{"cell_type":"markdown","metadata":{"_uuid":"c07c0e0f3d3ab59aac63473cf94a7fa1f44feacc"},"source":["We notice that the results of the `loss function` and `RMSE` functions gradually decrease after the loops. This shows that the `gradient descent` algorithm has been effective in reducing forecast error. However, sometimes we will encounter a situation where `loss function` and `RMSE` increase gradually. There are many reasons for this such as the `learning rate` and `regularization` coefficients being set too high causing the algorithm to jump out of the global extremes or it is also possible that the `loss functions` only increase temporarily and decrease then let the solution move through the local extreme point. For possibility 1 we need to adjust the `learning rate` and `regularization` factors so that the algorithm moves in the right direction towards the optimal solution. The second possibility is not too serious because the algorithm can move to the optimal solution immediately afterwards.\n","\n","Recommend the 10 most potential products for user_id = 200:"]},{"cell_type":"code","execution_count":25,"metadata":{"_uuid":"064cc8f6c7fb91e083bbdeaf442a544dca5681d9","execution":{"iopub.execute_input":"2024-04-25T16:32:32.895797Z","iopub.status.busy":"2024-04-25T16:32:32.895328Z","iopub.status.idle":"2024-04-25T16:32:32.900943Z","shell.execute_reply":"2024-04-25T16:32:32.900154Z","shell.execute_reply.started":"2024-04-25T16:32:32.895569Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Top 10 item_id recommended to user_id 200: [3943 3944 3945 3946 3947 3948 3949 3950 3951 3952]\n"]}],"source":["mf.recommend_for_user(user_id = 200, k_neighbors = 10)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T16:32:32.902745Z","iopub.status.busy":"2024-04-25T16:32:32.902324Z","iopub.status.idle":"2024-04-25T16:32:32.908093Z","shell.execute_reply":"2024-04-25T16:32:32.907295Z","shell.execute_reply.started":"2024-04-25T16:32:32.902568Z"},"trusted":true},"outputs":[],"source":["import subprocess\n","from ast import literal_eval\n","\n","def run(command):\n","    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\n","    out, err = process.communicate()\n","    print(out.decode('utf-8').strip())"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T16:32:32.909808Z","iopub.status.busy":"2024-04-25T16:32:32.909361Z","iopub.status.idle":"2024-04-25T16:32:33.148471Z","shell.execute_reply":"2024-04-25T16:32:33.147553Z","shell.execute_reply.started":"2024-04-25T16:32:32.909588Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["# CPU\n","model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n","cpu MHz\t\t: 2000.170\n","cpu cores\t: 2\n"]}],"source":["print('# CPU')\n","run('cat /proc/cpuinfo | egrep -m 1 \"^model name\"')\n","run('cat /proc/cpuinfo | egrep -m 1 \"^cpu MHz\"')\n","run('cat /proc/cpuinfo | egrep -m 1 \"^cpu cores\"')"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T16:32:33.150514Z","iopub.status.busy":"2024-04-25T16:32:33.150078Z","iopub.status.idle":"2024-04-25T16:32:33.229158Z","shell.execute_reply":"2024-04-25T16:32:33.228292Z","shell.execute_reply.started":"2024-04-25T16:32:33.150314Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["# RAM\n","MemTotal:       32880784 kB\n"]}],"source":["print('# RAM')\n","run('cat /proc/meminfo | egrep \"^MemTotal\"')"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T16:32:33.231324Z","iopub.status.busy":"2024-04-25T16:32:33.231004Z","iopub.status.idle":"2024-04-25T16:32:33.310917Z","shell.execute_reply":"2024-04-25T16:32:33.310106Z","shell.execute_reply.started":"2024-04-25T16:32:33.231265Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["# OS\n","Linux 4cee3a609c76 5.15.133+ #1 SMP Tue Dec 19 13:14:11 UTC 2023 x86_64 GNU/Linux\n"]}],"source":["print('# OS')\n","run('uname -a')"]},{"cell_type":"markdown","metadata":{"_uuid":"2d48f9ee5938292ccbcf314dc4c75330743fe7e1"},"source":["### 2.2.3. Build loss function with bias\n","\n","We find that in the estimation model there always exists errors that are not explained by hidden factors. For example, in a situation where a rather demanding user has rated a movie highly just because he feels that other users have also rated that movie highly or only in a user who is interested in a very small aspect of the movie such as actors. favorite, favorite saying or impressive song,... And it is obvious that these factors cannot be included in the model. Therefore, if we add a *bias factor* (bias) for each user prediction on a movie, it will make the prediction model more accurate. Bias factors can come from both the user or the item. We consider $b_m$ as a bias factor originating from the mth item representing effects on ratings that cannot be explained by the features of item m and $d_n$ as a bias factor originating from the nth user representing unexplained effects of user n's features. $\\mathbf{b} \\in \\mathbb{R}^{m \\times 1}$ and $\\mathbf{d} \\in \\mathbb{R}^{1 \\times n}$ are the column vectors and The line of bias factors comes from item and user. Then the `loss function` will look like:\n","\n","\n","$$\\mathcal{L(\\mathbf{I}, \\mathbf{U}, \\mathbf{b}, \\mathbf{d})} = \\frac{1}{2s}\\sum_{n = 1}^{N}\\sum_{m: r_{mn} = 1} (y_{mn} - i_m . u_n - b_m - d_n)^2+\\frac{\\lambda}{2}(||\\mathbf{I}||_{F}^2+||\\mathbf{U}||_{F}^2+|\\mathbf{b}||^2+||\\mathbf{d}||^2)$$ \n","\n","If we consider the matrix $\\mathbf{I}$ fixed, the problem is equivalent to optimizing the `loss function` of the form:\n","\n","$$\\mathcal{L(\\mathbf{U},\\mathbf{b},\\mathbf{d})} = \\frac{1}{2s}\\sum_{n = 1}^{N}\\sum_{m: r_{mn} = 1} (y_{mn} - i_m . u_n - b_m - d_n)^2+\\frac{\\lambda}{2}(||\\mathbf{U}||_{F}^2+||\\mathbf{b}||^2+||\\mathbf{d}||^2)$$ \n","\n","Considering at user n, the value of the loss function is:\n","$$\\begin{eqnarray}\\mathcal{L}(\\mathbf{u_n}, \\mathbf{\\hat{b}_n}) & = & \\frac{1}{2s}\\sum_{m: r_{mn} = 1} (y_{mn} - i_m . u_n - b_m - d_n)^2+\\frac{\\lambda}{2}(||\\mathbf{u_n}||^2+||\\mathbf{\\hat{b}_n}||^2) \\\\\n","& = & \\frac{1}{2s}||\\mathbf{\\hat{y}_n}-\\mathbf{\\hat{I}_n}.\\mathbf{u_n}-\\mathbf{\\hat{b}_n}-\\mathbb{1}(d_n)||_F^2+\\frac{\\lambda}{2}(||\\mathbf{u_n}||^2+||\\mathbf{\\hat{b}_n}||^2)\n","\\end{eqnarray}$$ \n","\n","Here we use the symbol $\\mathbb{1}(d_n)$ to represent a column vector consisting of all column values that are equal and equal to $d_n$\n","\n","Derivative at user n according to vector $\\mathbf{u_n}$:\n","\n","$$\\frac{\\partial \\mathcal{L}(\\mathbf{u_n}, \\mathbf{\\hat{b}_n})}{\\partial \\mathbf{u_n}} = \\frac{-1}{s} \\mathbf{\\hat{I}_n}^{T}(\\mathbf{\\hat{y}_n}-\\mathbf{\\hat{I}_n}.\\mathbf{u_n}-\\mathbf{\\hat{b}_n}-\\mathbb{1}(d_n))+\\lambda \\mathbf{u_n}\n","$$ \n","\n","Derivative at user n according to vector $\\mathbf{\\hat{b}_n}$:\n","\n","$$\\frac{\\partial \\mathcal{L}(\\mathbf{u_n}, \\mathbf{\\hat{b}_n})}{\\partial \\mathbf{\\hat{b}_n}} = \\frac{-1}{s}(\\mathbf{\\hat{y}_n}-\\mathbf{\\hat{I}_n}.\\mathbf{u_n}-\\mathbf{\\hat{b}_n}-\\mathbb{1}(d_n))+\\lambda \\mathbf{\\hat{b}_n}\n","$$ \n","\n","The updated formula *gradient descent* gives:\n","\n","* vector $\\mathbf{u}_n$:\n","\n","$$\\mathbf{u_n}' = \\mathbf{u_n} - \\theta(-\\frac{1}{s}\\mathbf{\\hat{I}_n}^{T}(\\mathbf{\\hat{y}_n}-\\mathbf{\\hat{I}_n}.\\mathbf{u_n}-\\mathbf{\\hat{b}_n}-\\mathbb{1}(d_n))+\\lambda \\mathbf{u_n})$$\n","\n","* vector $\\mathbf{\\hat{b}_n}$:\n","\n","$$\\mathbf{\\hat{b}_n}' = \\mathbf{\\hat{b}_n} - \\theta(-\\frac{1}{s}(\\mathbf{\\hat{y}_n}-\\mathbf{\\hat{I}_n}.\\mathbf{u_n}-\\mathbf{\\hat{b}_n}-\\mathbb{1}(d_n))+\\lambda \\mathbf{\\hat{b}_n})$$\n","\n","Similarly for the case of fixed matrix $\\mathbf{U}$:\n","\n","$$\\mathcal{L(\\mathbf{I},\\mathbf{b}, \\mathbf{d})} = \\frac{1}{2s}\\sum_{n = 1}^{N}\\sum_{m: r_{mn} = 1} (y_{mn} - i_m . u_n - b_m - d_n)^2+\\frac{\\lambda}{2}(||\\mathbf{I}||_{F}^2+||\\mathbf{b}||^2+||\\mathbf{d}||^2)$$ \n","\n","Considering item m, the value of the loss function is:\n","$$\\begin{eqnarray}\\mathcal{L}(\\mathbf{i_m}, \\mathbf{\\hat{d}_m}) & = & \\frac{1}{2s}\\sum_{n: r_{mn} = 1} (y_{mn} - i_m . u_n - b_m - d_n)^2+\\frac{\\lambda}{2}(||\\mathbf{i_m}||^2+||\\mathbf{\\hat{d}_m}||^2) \\\\\n","& = & \\frac{1}{2s}||\\mathbf{\\hat{y}_m}-\\mathbf{i_n}.\\mathbf{\\hat{U}_m}-\\mathbb{1}(b_m)-\\mathbf{\\hat{d}_m}||_F^2+\\frac{\\lambda}{2}(||\\mathbf{i_m}||^2+||\\mathbf{\\hat{d}_m}||^2)\n","\\end{eqnarray}$$ \n","\n","The updated formula *gradient descent* gives:\n","\n","* vector $\\mathbf{i}_m$:\n","\n","$$\\mathbf{i_m}' = \\mathbf{i_m} - \\theta(-\\frac{1}{s}(\\mathbf{\\hat{y}_m}-\\mathbf{i_n}.\\mathbf{\\hat{U}_m}-\\mathbb{1}(b_m)-\\mathbf{\\hat{d}_m})\\mathbf{\\hat{U}_m}^T+\\lambda \\mathbf{i_m})$$\n","\n","* vector $\\mathbf{\\hat{d}_m}$:\n","\n","$$\\mathbf{\\hat{d}_m}' = \\mathbf{\\hat{d}_m} - \\theta(-\\frac{1}{s}(\\mathbf{\\hat{y}_m}-\\mathbf{i_n}.\\mathbf{\\hat{U}_m}-\\mathbb{1}(b_m)-\\mathbf{\\hat{d}_m})+\\lambda \\mathbf{\\hat{d}_m})$$\n","\n","So in case the loss function has an additional bias factor, we also use the `gradient descent` equation to update the optimal solution. However, we will have additional solution updates for the bias factors and the loss function value will be influenced by the bias factors. Readers interested in the results of this algorithm can refer to [code](https://www.kaggle.com/phamdinhkhanh/matrix-factorization-movie-len-1m-with-bias?scriptVersionId=6262032). I already wrote it."]},{"cell_type":"markdown","metadata":{"_uuid":"905d687cc80cf2e0fbd331207f50f67fbc5b0830"},"source":["## 2.3. References\n","\n","1. [Recommendation System - Stanford](http://infolab.stanford.edu/~ullman/mmds/ch9.pdf)\n","2. [Collaborative Filtering Youtube - Stanford](https://www.youtube.com/watch?v=h9gpufJFF-0&t=436s)\n","3. [Recommendation System - Machine Learning - Andrew Ng](https://www.youtube.com/watch?v=YW2b8La2ICo)\n","4. [Matrix Factorization - Machine Learning Cơ Bản - Tiep Huu Vu](https://machinelearningcoban.com/2017/05/31/matrixfactorization/)\n","5. [Matrix Factorization techniques for recommender systems](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf)\n","6. [Learning from Incomplete Ratings Using Non-negative Matrix Factorization](https://archive.siam.org/meetings/sdm06/proceedings/059zhangs2.pdf)\n","7. [Matrix Factorization - Albert Au Yeung](http://www.albertauyeung.com/post/python-matrix-factorization/)\n","8. [Algorithms for Non-negative Matrix Factorization - Daniel D.Lee and H. Sebastian Seung](http://hebb.mit.edu/people/seung/papers/nmfconverge.pdf)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4486,"sourceId":6865,"sourceType":"datasetVersion"}],"dockerImageVersionId":11105,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}
